

------------------------------
Notes from Tue 27 Jun @work

I'm currently sitting in the CASG PBC course.





------------------------------
Notes from Fri 30 Jun

I've just been running the race results script for 2015. It crashes on occasion with the database connection error. This is annoying and I might next try running it from the laptop to see what happens.

I also had a crash on row 1076 of the 2015 race weblinks table, which corresponds to the following results:

http://www.cyclingnews.com/races/bpost-bank-trofee-koppenbergcross-2015/elite-women/results/

This results table (there's only one) has a mix of time and laps in the result column!! I've just skipped it at the moment.....

I've finished running all the results scrape for 2015.



------------------------------
Notes from Thu 29 Jun

Updates to race_results_tables_V2 function. Added text_clean to Country and moved database open and close script to where it is used.

So I worked out the issue with the non-UTF character problem (as usual, not writing to the database). I needed to run the text_clean function on the 'Country' column as well. This normally isn't required, as the 'Country' is normally a CAPS three-letter country code, but in the case of the Old Pueblo 24hr mountain bike race, the Country column contained the team names which were a range of weird and wonderful titles. This race also had the anomaly of 'laps' being used for the result column. I've decided to leave this alone (and not create an entirely new master table in the database for 'lap' result info) as I'm unlikely to ever need this data. It gets categorised as 'pts'. There are four-man teams in some of the tables, which means the 'Rider' column has four riders. Again, I'm not worried as I don't think I'll ever need data from races like these..... (famous last words!)

I'm still writing race results data to the 'test_test' tables in the database. Once I'm happy the function is scraping effectively, I'll do some testing on the results and hopefully the use of the data. Perhaps some prelimary summarising and plotting. If that works out then I'll sort out doing a clean scrape of all the race results data....

The Tour De France starts in two days. My target was to have analysis up and running and possibly even having a go at some predictions at this point. I'm a long way behind that, but it's still satisfying to be on the cusp of having the complete database of race results stored and ready for analysis. Phew!

It's hard to know what to do with the problems I'm having with the connection to the database. I'm finding it difficult to isolate what's causing the error. I feel like my database connection code is nice and clean. Anyway, I'm trying a fix where I've moved the database connection and close script to directly where it's used in the race results function. Opening and closing the connection exactly when needed. 

Next steps:

- Further testing of race results scraping function. Look out for database connection error. Once a large amount of data is scraped (perhaps a few years of results), test out the stored data for correctness and then use in analysis.

- Check Race Calendars. Combine into master calendar

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.






------------------------------
Notes from Tue 27 Jun

Firstly - I've changed how race time is handled by the race results table function. I've made the following adjustments:
a. I've decided to retain the original time entry from the website, in the original 'Result' column
b. I'm converting the time using lubridate::seconds (new column 'result_seconds') and then leaving the value as an integer (seconds)
c. Creating a new 'duration' column with the correct cumulative number of seconds for each rider.

I've addressed a variety of issues with the race results tables function and I'm sure I'll run into a few more use case issues with the new functionality I've added. 

I'm having problems with connections to the database (too many connections). I've fiddled with the close all connections script, but in the end I restarted the database.

I'd addressed an issue by extending the text_clean function to the 'Team' column and the captions and I inserted a gsub to remove the parentheses on the 'Country' column (occurs when there is no team).

Inserted IF script to deal with duration conversion on tables with only one row....

Next steps:

- Attempt to work out why I'm getting the following error:
 Error in .local(conn, statement, ...) : 
  could not run statement: Lost connection to MySQL server during query
It's weird because it doesn't look like there are too many connections and if I start running the script again (just prior to the error point) it continues on its merry way.

- Looks like there are more invalide UTF-8 issues with the race results tables function. Latest is at Row 175 of Race_Weblinks 2013.
http://www.cyclingnews.com/races/24-hours-of-old-pueblo-2013/results
This race is a MTB endurance race and has tables with 'laps' in them!!! Arrrghh!

- Check Race Calendars. Combine into master calendar

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.


------------------------------
Notes from Sat 24 Jun

I've spent the morning running the race weblinks function through the 2013 calendar entries. I've run into some problems, but it would appear the script wasn't dealing with them thanks to the updates I'd lost through merging to git.

Anyway - I think I've got it running again. I needed to insert two items back:

1. A logical test at the first IF statement to check the race url is longer than 7 characters. The error I've encountered is some race links are pointing to the head cyclingnews.com/races page, which is both live and contains an overload of stage data.
2. Some weird race tables are not binding with the race master. I decided to avoid having to come up with some complex solution for very rare (and uneeded) cases by inserting a tryCatch() around the rbind.

I've now got the race weblinks function running again for 2013.

Simultaneously I've been working on how to deal with durations in the master_time table. I wrote a small function at work designed to convert durations (stored as strings in the database) back into a time format. That seems to work, but I'm considering what the best method for storing will be long term. It might be preferable to just convert all durations to seconds (at the moment of source) and then convert to a numeric or interger, and then store them as a number in the database. 

Next steps:

- Integrate my new duration conversion into script accessing the database.

- Possible consider storing race times as lubridate::seconds instead of lubridate::duration.

- Check Race Calendars. Combine into master calendar

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.


------------------------------
Notes from Thu 22 Jun

I'm apparently still muddling through my use of git when it comes to multiple computers. This Daily Notes file is the best example as I managed to lose a couple of days notes when dealing with out of phase updates between the HP Laptop and Study PC. I didn't think I was working on different files or really even updating stuff at different times. Anyway - more learning there which I'll defer until after I've got the full dataset scraped for the first time. 

I believe I've sorted out converting durations stored in the database back to the correct class/format. Sadly, I haven't discovered an elegant function that does this easily. It's a bit frustrating that a lubridate function doesn't do this. There's some irony in the fact lubridate recongnises time/date formats in strings in so many formats.... other than it's own duration!!

My new script for dealing with duration is in the following working file:

170622 - Working with lubridate duration.R

Next steps:

- Integrate my new duration conversion into script accessing the database.

- Possible consider storing race times as lubridate::seconds instead of lubridate::duration.

- Scrape 2013 race weblinks data

- Continue working on how to record error and warning outputs (tryCatch) (I think I've got this going)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.




------------------------------
Notes from Sat 17 Jun

I've updated the race weblinks script to include the tryCatch() function. This is meant to create a method of capturing errors in an error log file (weblinks_error_list.csv) and allow the script to continue scraping further weblinks.

I've just watched the script run into a warning at row 21 of 1274 for 2013 and it's printed my warning script to the console and continued to the next row. I've just checked the error log and it's empty! Time to check whether I create a blank error log everytime a new row is started.....

Upon running the new function for 2013, it's caused R to abort when running Row 43. I've just it a second time and the issue happens at exactly the same point. This is frustrating as it's very difficult to determine what is causing the abort.

I've looked at the diagnostics reports (which have nothing useful) and tried emptying the Global Environment. I really don't think it's a memory issue as the system memory is at less than half capacity during the script execution.

I'm now doing an update to both R and R Studio to see if this helps.

Next steps:

- More to learn on merging git branches.

- Work out how to convert back Duration values stored in the database results tables (currently a character).

- Continue working on how to record error and warning outputs (tryCatch) (I think I've got this going)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.






------------------------------
Notes from Thu 15 Jun @ work

I've had a crack at improving my understanding and use of the tryCatch() function and made some progress. The solution to capturing the error/warning apparently seems to be putting a robust file access solution into the tryCatch() function, in this case accessing a .csv file and then writing new error/warning message to this file.

Next steps:

- Sort out why it's only recording the first warning message (but still recording the year and error counter numbers just fine. It's recording <NA> of the rest. 

- Looking at extending this to my TDF webscraping functions.




------------------------------
Notes from Thu 15 Jun

A fairly unproductice morning. A script I was running last night for the race results tables for 2010 was interrupted. Then the computer restarted. I'm not sure where that script got up to (I guess I can check in the database) and the computer took ages to reboot completely. I started using the laptop and then this lead to challenges merging my GitHub files. I spent most of the morning learning more about merging and recovering files on the git. I've got more to learn!

It is positive to have so many results now populating my master_results_time and master_results_points tables in the database. I did a quick check and the master_results_time table has 827,531 rows/entries!! (And I'm only up to 2010). I did a quick dplyr filter on Richie Porte (which worked) but then it became quickly clear I need to work out how to convert the time values back into the correct class as they are stored as a character, in the unique lubridate format. 

Next steps:

- More to learn on merging git branches.

- Work out how to convert back Duration values stored in the database results tables (currently a character).

- Continue working on how to record error and warning outputs (tryCatch)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Wed 14 Jun

I'm running into a number of issues trying to close out my race weblinks scrape and completing writing them to the database. The most troubling one is RStudio aborting in the middle of the race weblinks script running. 

I solved a more minor issue today. The script stopped with an error on a webpage containing a funny mix. It had a table with past winners of the race (not a table with stages and race results weblinks) and then the stages in the usual webpage format. So my script isn't built to deal with tables on the page for another purpose.

Anyway - I think these will be in the vast minority. This race was the USA cyclocross championships in 2016. I've decided not to try and solve this specific issue and instead implemented the use of the 'try()' function. I wrapped two 'try()' functions around "Fork One" and around "Fork Two" do deal with errors being thrown out. Ideally I want to use 'tryCatch()' instead but I've not yet worked out how to write the location of errors to table. Essentially my own version of an error log. This would allow the script to progress without stopping for errors, but then giving me details of the error location at the end so I can go and fault find. I'm not so worried about the race weblinks function, as I think I've got all of the needed races, but I want to make sure I've got a 100% solution for the race/stage result tables scraping function.

The race weblinks function is now running for 2013. I have 2011, 2013 and 2017 to complete. I'm working from home this morning and will hopefully be able to run these while I'm working.

Next steps:

- Continue working on how to record error and warning outputs (tryCatch)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Tue 13 Jun @ work

I've been experimenting with the TryCatch() function, which has been going okay. I've not yet worked out how to record the errors (to a dataframe). Apparently any executed work within the error function stays inside.

The good news is that with either 'try' or 'tryCatch' I should be able to run long webscraping scripts and have them continue, even if they run into an error. 


Next steps:

- Continue working on how to record error and warning outputs



------------------------------
Notes from Tue 13 Jun

I've just re-run race weblinks for 2011. It needs checking.


------------------------------
Notes from Mon 12 Jun

I ran into problems on Sunday with R crashing (aborting). I haven't had time yet to work out what's going on. It crashed in 2011 and I think in 2013 and 2016. I've written a short bit of script to return the status of race weblink tables from the database. Non-complete tables are obvious because they have only 5 columns (instead of the new 8 column format) and don't  have a full completment of rows.

I think the 2016 run of the race weblink function may have aborted on row 412.

Years to be run for the race weblinks function are: 2011, 2013, 2016 & 2017



------------------------------
Notes from Sun 11 Jun

The latest error kicked up was thanks to one (of ten) tables on a results page missing the column name '#' on the first column. I've decided to do away with the smarts and force the columns names to "#", "Rider Name (Country) Team", "Result". I figure even in the rare occasions this is different, ultimately I want uniformity so the tables will bind correctly and then upload to the database. 

These might be famous last words - but I think I'm close to being ready to scrape the full set of race weblinks and then the full set of race results tables shortly there after. 

### I was running from 2006 to 2012 and hit my first error in the middle of 2009, which is pretty good really. The issue was an 'Article 404' webpage. I've inserted 'RCurl::url.exists(race_url)' into the 'stages' check, which will hopefully sort it out.




------------------------------
Notes from Sat 10 Jun

I've been working on the race results tables function. I encountered an error last time at row 24 of the 2014 weblinks table. The use case error could be traced to a non-standard column for the Rider Name (Country) Team. In this case it was the Australian national championships and the column was titled 'Rider Name (State)' with no 'Team'.  I've decided to force the column title to match all the others, firstly to ensure uniformity across all the race results tables and to make sure these tables are combined and loaded up to the database without issue. 

Next steps:

- It would be really good to sort out the trycatch() function so I can run long web scrpaes and have the errors returned for sorting out afterwards.

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.




------------------------------
Notes from Thu 08 Jun - Evening

I've just tried running the get weblink function for calendar year 2010. It's immediately run into some new use-case errors:

- Non-complete race weblink data, resulting in less race weblinks than stages.
    example at row 14: http://www.cyclingnews.com/races/czech-cyclo-cross-championships-cn/stages/
- Error 404 non-existing page
    example at row 39: http://www.cyclingnews.com/races/croatian-cyclo-cross-championships-cn/stages

I came up with butcher's solution for the first one - repeating the list of results weblinks to fill the number of stages. I figure this will only occur on rare occasions for smaller races. The end result will be by result scraping function will still extract all of the available race result information. The bad news is that the result weblink may not always perfectly line up with the correct stage information....

The second issue is what I need to work on next, and coincidentally will likely require the 'trycatch()' function I was planning on implementing anyway. There appears to be a specific guide on using trycatch() with Error 404 problems at the following link:

https://protocolvital.info/2016/04/21/404-error-handling-with-rs-download-file/

Next steps:

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Improve scraping code to identify errors and report them. Function 'trycatch()' might be an option.

- Work out a secure connection process for accessing the database from offsite.



------------------------------
Notes from Thu 08 Jun

So it looks like I've got mostly operational functions to go and get all of the data I require. I started running the raec weblink function for Year 2012 and ran into an issue where the (number of) stage weblinks does not match the number of stages in the Stage table. I found the table had a duplicate stage and I inserted a line of code to fix this issue. I'm now running Year 2012 race weblink again.

I've realised it is important to learn how to write webscraping script that collects and returns data on errors and (if possible) continues running. Essentially, if I'm running through a loop and it fails, so collect identifying information for the failing row and then to continue to the next one.

Need to check 2012 race weblinks completed successfully.


Next steps:

- Check Race Calendars. Combine into master calendar

- Check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Improve scraping code to identify errors and report them. Function 'trycatch()' might be an option.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Wed 07 Jun - Sunrise session

I've firstly dealt with the error generated at row 746. 

 Error in matrix(NA_character_, nrow = n, ncol = p) : 
  invalid 'ncol' value (too large or NA)

I discovered the last table in the following page is empty
http://www.cyclingnews.com/races/prudential-ride-london-classic-2015/ridelondon-classic/results/

Despite being empty, the annoying table has the sub-node 'tbody', which was my previous discriminator for eliminating empty tables. I've now extended the xpath identifier to contain '/tr' as well:
xpath="//table[.//tbody/tr]"

I'm a bit worried this might exclude some tables I want to keep, if they are missing the '/tr', but I actually doubt there are any of these.

I'm now running the rest of the race calendar. I will, however, have to carry out thorough accuracy testing. I think it actually works! Lots of testing to do yet.....

Next steps:

- Work out a secure connection process for accessing the database from offsite.

- Look at combining the race weblink table into a single master table.

- Work on combining the calendar tables into a master table. 

- Work out a secure connection process for accessing the database from offsite.




------------------------------
Notes from Tue 06 Jun - End of day

The race result table script has been running relatively well. The latest error is at row 746.


------------------------------
Notes from Tue 06 Jun - Midday

I inserted some code dealing with completely empty 'Result' columns, including no 'Result' column name. Now we're getting somewhere. I started the complete calendar test again and reached row number 85, which is the following event result:

http://www.cyclingnews.com/races/etoile-de-besseges-2015/stage-2/results/

The issue is almost certainly that the Sprint 1 table has no rows (after the columns headings), because the sprint was nulified. I'm sure we'll come across this again. So I need to sort this use-case out too.


------------------------------
Notes from Tue 06 Jun - Midday

Going okay. I sorted the early error, but have now run into the special case of race results tables with no times or points. In this case the track world championship Keirin event!
http://www.cyclingnews.com/races/uci-track-world-cup-iii-2014-1/day-1/results/

I think the lubridate duration function doesn't like being passed NULL values. 


------------------------------
Notes from Tue 06 Jun - Sunrise session


I think I've a reasonably elegant solution to the task of only extracting tables with content. By modifying the xpath selector I'm able to only extract table nodes with the correct child element, in this case a 'tbody' node. 

html_nodes(xpath="//table[.//body]")

So I started running tests and got plenty of problems, but I think I've managed to sort most of them. I'm about to run a bigger test on a whole calendar year, while we go and have breakfast. I anticipate problems!!

I just started it and got my first error stopping everything - at row 6!


------------------------------
Notes from Mon 05 Jun

I think I've sorted the issue of dodgy webpages. I ran my test again and it progressed past the nasty extra 2012 Paris Roubaix race. 

Next problem:  Some result pages apparently have empty table nodes. The one that's causing me grief at present is:
http://www.cyclingnews.com/races/gp-de-denain-porte-du-hainaut-2015/results

It returns to tables (even though there is a single results table on the page). The first table node is empty, and is causing an immediate error in the rvest 'html_table' function towards the top of the function_race_results_table_V2 function.

Next steps:

- Sort out a method for eliminating the empty table.

- Continue on the Next Steps list below



------------------------------
Notes from Sat 03 Jun

I've hopefully solved some interesting problems today.

The main one was I found and issue writing to the database and traced it to results pages not containing and 'points' results. The one I came across was for the Ronde van Drenthe race in 2015 which only has a single results table, for finishing time. This means the 'points_tables' is empty (NULL) and the dbWriteTable function spits the dummy when presented with no data. This was easy enough to solve with an appropriate 'IF' statement. Good to know, as there will be plenty of these results across the calendar.

The earlier problem metastasised when I was dealing with non-breaking spaces in the 'Result' and 'result_type' columns. I (had previously) inserted a clever

I've run a longer test and I think I've come into the issue of a dodgy URL link:
http://www.cyclingnews.com/races/paris-roubaix-2012/results

The above link has a number of issues. Firstly (and most importantly), it's got the full URL and not just the bit after 'cyclingnews.com'. The second issue is it's for the 2012 Paris-Roubaix and this URL is in the middle of the 2015 Race Weblink table.

Anyway, I think I need to put in a test for URL EXISTS going forward. I can't yet find one that weeds out clearly non-functioning websites....

Next steps:

- Identify and implement a robust url checking decision process

- A full run of the race weblinks functions for all calendar years. (This will take some time!)

- Work out a secure connection process for accessing the database from offsite.

- Look at combining the race weblink table into a single master table.

- Work on combining the calendar tables into a master table. 

- Work out a secure connection process for accessing the database from offsite.



------------------------------
Notes from Thu 01 Jun - Day Session (WPD)

I'm about to take a break to go and pick up my wheel from Tom Wallace Cycles. I'm having problems with the race results tables function, I think due to issues clearing the database connections...


I've since identified that the issue is the 'Result' column is being created as a list. I had a solution that turned it into an integer, but then this wrecked the 'time' based Result values.

I'll need an IF statement of some type.





------------------------------
Notes from Thu 01 Jun - Sunrise Session

I'm running a test of data from the new race weblinks data being fed into the race table extract. I'm getting a number of issues:

- Problems writing to the database with the updated race table data
- Issues with my script splitting 'points' tables from 'time' based tables.

Next steps:

- First I'm working on sorting out the 'points' vs 'time' split



------------------------------
Notes from Tue 30 May

I've made further refinements to the race weblinks function to correct errors and improve the outcome. 

The main initial error not having 'dplyr' called as a required package at the start of the function. The first time the function was loaded cleanly and needed to 'filter' some rows the code threw out an error that was initially hard to identify. Having 'rvest' loaded meant the piping was working, but the dplyr specific functions were not. I guess when I was writing and testing development of this function I probably had dplyr loaded into the environment. It's definitely been useful working from several computers and getting the code to robustly execute.

Improvements included pulling through the race location from the calendar file for stages where no location data is scraped from the race or stage webpage. I also inserted the 'text_clean' function to uniformly sort out location data.

There was already the usual include script for a delay between webscraping. 

I now need to look at combining the race/stage weblink data for individual years into a master table. This should be able to occur immediately as I think each row entry is already unique and has a PRIMARY KEY column (stage-id). With the dates included for every row, it will be open to immediate filtering and analysis.

I've just run a test of the function using the full 2015 race calendar. I'll save this Daily Notes file before I see the end result.

Next steps:

- A full run of the race weblinks functions for all calendar years. (This will take some time!)

- Work out a secure connection process for accessing the database from offsite.

- Look at combining the race weblink table into a single master table.

- Work on combining the calendar tables into a master table. 


------------------------------
Notes from Mon 29 May @ work

Further testing and refinement of 02_function_obtain_event_race_results_weblinks.R.

Some of the issues I've dealt with include:
* Some race pages not having any results at all (!!) - e.g. Milan San Remo 2015
* Some race pages with result links not having a date (at least in the right format)

Next steps:

- Insert time delay script for webscraping

- Further testing and fault finding on 02_function_obtain_event_race_results_weblinks.R

- Work out a secure connection process for accessing the database from offsite.



------------------------------
Notes from Sat 27 May

I believe I've got the race weblink extraction function (#02) reasonably functional. I need to do some more testing.




------------------------------
Notes from Fri 26 May @ work

I've made some improvements to the race weblinks function that essentially deal with the scenario where race_links = 0, i.e. there are no race weblinks. Some 'if' statements did the trick.

I've just tested the function again and it's improved, but it's encountered an error with the 2015 TDF. It can't see the column "results" in the extracted table. I need to look at whether it's picking up an incorrect table as the one loaded in has overall race results....

Next steps:

- Further testing and fault finding on 02_function_obtain_event_race_results_weblinks.R

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Thu 25 May @ work

I made good progress on developing the race weblinks function and think I have it close.
Initial testing has been positive. I've run into an early issue with testing on the 2015 TDF.
It appears the race link for the TDF goes to some form of "countdown page":
# http://www.cyclingnews.com//races/2015-tour-de-france-countdown-2015
This appears to be more of an issues for the race calendar function than the race weblinks function.
Anyway - I'll keep testing and see where I get to.

Next steps:

- Continue testing on updated race weblinks function: 02_function_obtain_event_race_results_weblinks.R

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Thu 25 May

I'm making progress on the 02_function_obtain_event_race_results_weblinks.R file. I've inserted IF/ELSE statements and the code to deal with date formats in both table and non-table arrangements. At the moment I'm sorting out the non-table format.

Next steps:

- Finish insertion of new code into race result weblink function dealing with alternate storage of date data. Test and debug.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Wed 24 May @ work

I made some updates to the following test script, in support of sorting out the race weblinks function

170523 - Test script for extracting race weblinks.R

I managed to clean up the table-based data extraction:
- Deleting Rest Day rows
- combining table with weblinks


------------------------------
Notes from Tue 23 May @ work

Just when I thought I'd licked this one I discover and issue and open a can of worms. Of course data for multi-race/stage races are stored in different formats. The main obvious difference is that stage races (Grand Tours, etc) are stored in a table, whilst multi-event races (such as National Championships) are stored in a divided page layout.

Next steps:

- Continue work I'm progressing on extracting the data I need from table-based pages.

- Create some sort of test (if statement possibly) to distinguish between the two types.

- Carry out testing. I possibly need to send a race calendar file to work.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Tue 23 May

I managed to fix the main issues with 02_function_obtain_event_race_results_weblinks.R regarding the date extraction. I inserted a suitable bit for single dates (I needed to adjust the html reference and adjust the format of the lubridate conversion to handle dates with a time as well as a date).

Next steps:

- Do a bigger test of the improved 02_function_obtain_event_race_results_weblinks.R function.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Sat 20 May

I carried on testing and improving the race result weblink function. 

Still working on the following function:
02_function_obtain_event_race_results_weblinks.R

I'm running a limited test (10 rows from 2011) and the function is not returning a correct table. It's not returning the target year data and it's missing the new stage_date column

Next steps:

- Assess changes to race weblink extract function and whether dates (and raceweblinks) are being pulled in accurately). 

- Fix missing single date races and ensure dates are being converted to correct class.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Fri 19 May @ work


I did some mild messing around with the race weblinks script, but spent most of the time trying to sort out the issues I had with RStudio freezing on opening.

The good news is that the git process appears to be working well.


------------------------------
Notes from Fri 19 May

I'm getting the race weblinks function updated to pull in the stage dates as well. There are a couple of challenges I've identified. 

The first is the stage dates are apparently displayed only once on the web page, but the race weblinks are usually duplicated. This means when I combine the two sets of lists into a dataframe, they are difference lengths. At the moment the dataframe stops building after it runs out of dates and (as far as I can tell from a short test) I end up with the correct table. However this is a bit of a butcher's method and I don't like it.

The second problem (I'm not certain it's a problem yet) is what happens when there are no race weblinks. I think the stages just get ignored, but I'm a bit worried the previous races' dataframe will be pulled in. 

The first test (for 2011) has just finished and it largely looks successful. The most obvious issue is that single date races (i.e. not stage races) don't have a date. This is probably because a date isn't listed on the race webpage. Dates are also in a straight numeric class. 

Next steps:

- Assess changes to race weblink extract function and whether dates (and raceweblinks) are being pulled in accurately). 

- Fix missing single date races and ensure dates are being converted to correct class.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Thu 18 May - at work

First clone of TDF_Predict git repository from work computer.

Next steps:

- Ensure git is working correctly.


------------------------------
Notes from Thu 18 May

I was away in Canberra for the first part of this week. I did not have any success remotely connecting to my database on the home Study PC.

I've started working on the "02_function_obtain_event_race_results_weblinks.R" function to pull in stage date information so it can be used later in the race result table.


Next steps:

- Continue working on modification of race weblink function to pull in stage date information.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Sat 13 May

This morning I've worked on allowing remote connection to my MySQL database. The primary actions performed were creating a function that sets the location of the database password file on both the Windows PC and the Linux laptop, and to modify the race_results_tables_V2 file to point to the results of this new function:

Next steps:

- Test remote connection from outside of home WiFI to see if I'm going through the firewall okay.

- Start working on testing webscrape of race results tables.


------------------------------
Notes from Thu 11 May

Today I've been continuing to work on the race result tables. I'm attempting to work out how to build the time-based 'Result' column into the correct race duration for each rider. I successully eliminated the 'non-breaking space' characters from the Result column, but I'm now trying to work out how to mutate a new Duration column with the correct cumulative time for each rider.

I've been able to correctly assign new columns for race_type and race_classification.

Next steps:

- Continue working out how to build a 'Duration' column

- Correct non-finishing entries (e.g. DNF, DNS, DNQ, etc).

- Combine all 'time' tables and 'points' tables into two master tables.


------------------------------
Notes from Wed 10 May

Wet weather in Brisbane this morning, so no 4501 Rouleurs River Loop. Hopefully it will be dry enough for me to ride into work later.

After doing the Giro daily update, today I worked on further sorting out the race result tables.

The cool bit I worked out today is how to split a combined column (in this case "Rider (Country) Team" into separate columns using the 'separate' function from Hadley's 'tidyr' package. It took a bit to understand the syntax and identify the correct regex expression for matching special characters (in this case the parentheses). 

Next steps:

- Combine the script I developed today to separate columns with the other new script I've developed recently to use rvest to download race result tables, correctly assign headers and assign columns for pts/time and race_id.


------------------------------
Notes from Tue 9 May

I've been distracted by getting daily updates for the Giro Velogames competitions up and running, which is now going well. Time to concentrate on the main game!

I believe I've got a format and solution for extracting and storing race result information. The two big decisions are:

1. I will store all of the TIME based results in one table, and all of the POINTS based results in another table.
2. Each table entry will hace its 'result class' (full, general_classicification, sprint, points_classification, etc) stored in each row (in-line) as a field.  There will also be a field for 'result_type' which will simply state whether the numerical result is a 'time' result or a 'points' reult.


So this means all of the time based results (full stage result, GC, teams, young rider, etc) will be thrown into one big table. The results will be separable via the race_id (also a field) and the result_class.  




------------------------------
Notes from Thu 29 Apr

I've started looking at how to sort out the storage of race result tables. I also today identified it is neater/easier to use the 'dbGetQuery' function instead of using the two-step dbSendQuery and dbFetch. Apparently the above is recommended by Hadley.


Next steps:

- Assess how race result information will be called. If I'm going to combine all the race result tables into a Master table, additional columns will be required to ensure results are unique. The most obvious addition is the unique 'stage_id'.

- Cleaning up the race result data. For example
	* Filling in empty time entries (making sure I don't give a time to DNF, etc
	* Separating data squashed into a single column (Rider Name, Country, Team, etc)






------------------------------
Notes from Thu 29 Apr

I've started sorting out the race results function and working out how this is all going to get stored on the database. First step was trying to execute the race_results_tables function, which has a variety of errors.

I found that my script in 'Test script for running race results' was referencing the incorrect columns from the 'race_weblinks_Master' dataframe.

Anyhow, I've got that sorted, as well as cleaning up the race_results_tables function a bit. I'm still a bit stuck on how to combine the variety of tables into Master results tables.

Next steps:

- Look at how to combine race results data into master tables

- Think about how I'll retrieve race data. Dates may be a challenge if the date isn't stored in the race results tables.


------------------------------
Notes from Thu 27 Apr - PM

I've re-run the race_calendar function successfully and now have a race_calendar_Master table in the database. It appears to have clean race details and locations as well as dates in a useable format. I've done the same for the race_weblinks tables, which I believe has run successfully. I now have a race_weblinks_Master table in the database as well!

Next steps:

- Move on to race results tables. This is a huge challenge and I'm not exactly where to start.
  I need to work out how I'm going to store and retrieve information from these tables for analysis.


------------------------------
Notes from Thu 27 Apr

I'm having more problems than I anticipated sorting out the race_calendar_Master table. I've discovered a couple of challenges:

1. I haven't done a consistent update to the race_calendar tables and as a result there is inconsistent column naming. At this stage I know the earlier ones are named 'Start.Date' and the more recent tables have 'start_date'. I think I need to re-run the race_calendar initial extract function again.

2. The above might solve the following issue. I've converted some of the start_date and end_date values as a test, and then attempted to lubridate a master table with a combindation of converted and not-converted dates. lubridate::dmy returns an 'NA' for dates that have already been converted.

Next steps:

- Re-run the race_calendar initial function (01) for the tables with old column naming conventions (or just re-name the columns!)

- Combine all the race_calendar tables into a single race_calendar_Master table

- Look at how to combine race results data into master tables



------------------------------
Notes from Tue 25 Apr

I attempted to close out my Coursera exercises, but my access to Jupyter Notebooks has been cut off! Well, at least direct access via Courera has.

I've moved on to sorting out my database, starting with the race calendars and combining them into a master calendar. I think they're almost ready to go, I just need to ensure when they are mashed together that I can separate years cleanly. I discovered the dates are stored as a string and won't therefore responsd to numeric queries. Fortunately, Hadley's 'Lubridate' package was able to convert the 'DD MMMM YYYY' dates to a proper date numeric using the 'dmy' function. I also found that Lubridate's 'month' and 'year' functions are great at extracting elements of the numeric date entry, with formatting for taking out exactly what you want (for example - getting 'January' or 'Jan' instead of '01'.

I also discovered a MySQL function for closing open queries, but leaves the connection to the database and table open:

# Close open queries (doesn't close the connection - very useful)
dbClearResult(dbListResults(conn_local)[[1]])


Next steps:

- Perhaps write the Lubridate conversion straight into the initial race_calendar scrape function (01a_function_initial_CN_calendar.R).

- Check the data in the existing race_calendar tables

- Convert all the start_date and end_date values in the existing calendars (although I could do this when they're all combined)

- Combine all the race_calendar tables into a single race_calendar_Master table

- Look at how to combine race results data into master tables 


------------------------------
Notes from Thu 20 Apr & Sat 22 Apr

On both days I've simply progressed through the Week 4 exercises on my MySQL Database course.


------------------------------
Notes from Tue 18 Apr

I've mostly continued on the datbase week 4 exercises. At the same time I ran the obtain race weblink function to complete the final years (2016 and 2017[partial]). It's now possible to combine all of the race weblink tables into a single master table - noting that I would need to write and update script to go and get new race weblinks as the year (currently 2017) progresses.

I'm now working on the script that takes the raec weblinks and goes and scrapes all the race result tables. The biggest challenge is working out how I'm going to store this in the database. Ideally I would like a single master table for the primary race result, another master table for various points outcomes and that sort of thing. In theory I could ignore GC tables (as I should be able to calculate this from individual stage times), but it's probably prudent to collect this data for validation against my own calculations later on. There's almost certainly going to be some weird anomalies in GC times thanks to corrections and adjustments by race officials.

Next steps:

- Complete database Week 4

- Combine race calendars into a master table

- Look at how to combine race results data into master tables 


------------------------------
Notes from Mon 17 Apr

On Saturday (15th) I tried remote connecting from Maleny (Phil & Brenda's house) but failed. The Study PC (database server host) was suspended when I got home so I think I didn't really have the ability. Need to test again when I'm sure the computer is still on!

This morning I believe I've sorted out the 'text_clean' function. I've inserted the 'magrittr' function to allow me to pipe my various text operators into each other. I'm now running a complete re-scrape of the race calendars (2005 to 2017). It is important to have this little function sorted as I'll be using it repeatedly when I import race data.

I've also done a little bit on Week 4 of my database course. I need to close this out. Week 5 to do as well.

Next steps:

- Complete database Week 4

- Combine race calendars into a master table

- Look at how to combine race results data into master tables




------------------------------
Notes from Thu 13 Apr

I think I've successfully set up the ability to remotely connect in to my MySQL server. The laptop connects. I need to test it offsite to see if there are firewall issues.

Study Computer IP Address
192.168.1.1   
Port  3306

Next steps:

- Test remote connection to database (from Maleny)

- Start implementation of new database relational schema



------------------------------
Notes from Thu 06 Apr

Haven't been able to replicate the error from Tuesday. It was possibly just a HTTP connection error. I should look for code that test for a connection and is able to move on if there isn't on (possibly producing a warning). I think there's some in the UnConf::Ozdata work.

URL check courtesy of the RCurl package
   RCurl::url.exists(race_url)

I've inserted the above RCurl function into GetRaceWebLinks to test it out.

The following does a check. Returns nothing if TRUE. Returns and error if FALSE
   assertthat::assert_that(nrow(calendar_cn) < 0)

Next steps:

- Sort out remote connection to database

- Design database relational schema


------------------------------
Notes from Tue 04 Apr

I seem to have the 02_function mostly sorted. I've just tried running it (getRaceWeblinks) over 2005:2017, but it failed in the middle of 2009 (44% mark), with the following error:

Error: failed to load HTTP resource

The good news is that I sorted out plenty of errors, cleaned out a lot of uncessary code (from 02_function) as well as setting up and single text_clean function to uniformly deal with special characters.


Next steps

- Work out why 02_function failed on the 2009 calendar.



------------------------------
Notes from Mon 03 Apr

I spent this morning updating the calendar scraping function. The latest issue appears to be with another special character, this time the uptick (\91) {there has to be an official name for this problem character} contained within the 2011 calendar for: The Nor\92easter \91Cross 2011 whic is in September.

[UPDATE!]: I believe I've sorted it. The character is called 'Okina' (Hawaiian) and can be removed with the following stange regex expression:  [^[:alnum:]///' ]

gsub("[^[:alnum:]///' ]", "", races_master[i, "race_details"])


I inserted the text progress bar into the calendar and obtain race results functions as well.

Next steps

- Sort out how to deal with this latest problem special character.

- potentially stick all of the 'cleaning' actions into a single function, so this can be used uniformly across all of my cycling code.




------------------------------
Notes from Sun 02 Apr

I've returned to work on my TDF_Predict project after taking a week to focus on the BURGr UnConf 2017. The event was great and I learned an enormous amount. 


This morning I finished updates to initial calendar function and I believe I have all of the race calendars stored in the database.

I'm now moving on to the race results weblink function, which has errors. That's a job for tomorrow.





------------------------------
Notes from Thu 23 Mar

Same again - still working through Coursera database course. It's been really beneficial. Today I finished off my writing some queries in R looking at my Pro Cyling database. Everything seems to be working well! My practice file is:

170323 - Practice ProCycling Database Queries.R



------------------------------
Notes from Tue 21 Mar

Today I continued working through the database course on Coursera. Last week I learnt about Entity Diagrams and Relational Schema (how to interpret and create them) and this week I am getting into MySQL commands.



------------------------------
Notes from Mon 20 Mar

Super painful. I'm trying to sort out the non-UTF8 character problem still. In the 2017 race calendar, the first problem race is on row 14:

"Challenge Mallorca Trofeo Porreres \96 Felanitx \96 Ses Salines \96 Campos"

The problem is the 'em-dash' which isn't apparently UTF-8. I've built a replacement into the 'removePainfulCharacters' function, but I don't like it. Even once I've done that, the database write function throws up the same error - apparently picking out the " " space in the same row 14 string. Annoying!!

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R', specifically trying to sort out further non-UTF8 characters.

- Create an ER Diagram and Relational Schema for the Pro Cycling database.







------------------------------
Notes from Thu 16 Mar

I haven't made a great deal of progress with fixing the problem characters in the 'race_description' column of the calendar tables. I've decided to insert the 'try()' function at the start of the 'dbwriteTables' function and run through the remainding calendars. There's still plenty of problem characters. 

I did spend a bit of productive time familiarising myself with Hadley's lubridate package, which helps with working with dates. I've written a quick function 'setDateFormat' to correct and update the 'start_date' and 'end_date' columns in the calendars. 

I also had a quick look at Hadley's 'broom' package, which is used to clean up the output from predictive modelling. It neatly arranges output models into tables which can be used for further analysis and easier visualisation.

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R'



------------------------------
Notes from Mon 13 Mar

I'm still having problems with certain characters in the race_details column. I fixed the apostophe issue by creating a copy of the removeDiscritics function. In the 2009 calendar I've now run into a problem with inverted commas (") in the 253rd column: 3 Giorni  Internazionale open "Citt\E0\A0 di Pordenone"

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R'




------------------------------
Notes from Mon 13 Mar

I'm continuing to have problems sorting out characters in the calendar function. The first problem event is the following in the 2009 calendar, but there are more. I think the issue is with special/reserved characters, such as quotes and inverted commas. 

Tour of America\92s Dairyland
<a href="/races/tour-of-americas-dairyland-ne">Tour of America\92s Dairyland</a>

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R'


------------------------------
Notes from Sun 12 Mar

I had a short play with dplyr this afternoon while Liz was out for a run. The big success was working out how to summarise a value using multiple factor variables. The key was listing a number of variables in the one 'group_by' function dplyr.

e.g.

colnames(df) -> c("State", "Product", "Month", "Sales")

my_summary -> df %>%
   group_by(Month, Product) %>%
   summarise(sum(Sales))

knitr::kable(df)

Here's the list of packages from Hadley's slide presentation:


Import: readr, readxl, xml2, DBI

Tidy: tibble; tidyr

Transform: dplyr, forcats, hns, stringr, lubridate

Visualise: ggplot2

Model: broom

Program: purr, magrittr



------------------------------
Notes from Sat 11 Mar

I've needed to take a step back. I've realised I need the complete list of race calendars written to the database. This helped me realise that my calendar functions (01a & 01b) were writing .csv files instead of the database. I've cleaned these both up and I'm now running the scripts to extract the full race calendars with race weblinks and write them to the database. 

It's interesting to see my script from just over a month ago. It's evident I'm making progress in terms of writing more efficiently and in noting what I'm doing. 

I'm having problems with the '02_function_obtain_event_race_result_weblinks.R' file

I've enrolled in a Coursera course on 'Managing Big Data with MySQL':
https://www.coursera.org/learn/analytics-mysql/home/welcome

Next steps

- Continue working on problems with '02_function_obtain_event_race_result_weblinks.R'

- Continue working through race results functions. 

- Look at running 'GetAllRacesInAYear' function and scraping all years to database. Need to check on the format and data being written to the race weblinks tables.

- Consider how the tables might be amalgamated. Not easy given their heterogeneous nature. 













------------------------------
Notes from Thu 9 Mar


I had a quick go at using some of the dataframe arranging functions in the dplyr package. I went to my first BURGr Meetup last night and had a great time.
(tidyverse, magrittr, dplyr)
This is of course massively easier than some of the base R filtering techniques I've been using!

I was able to fix the issues with the 03_function_race_results_table.R' which is called out as the 'write_race_results_tables' function. I now need to go and scrape all of the race calendars from Cycling News using the 'GetRaceWebLinks' function from the '02_function_....' file.

Next steps

- Continue working through race results functions. 

- Look at running 'GetAllRacesInAYear' function and scraping all years to database. Need to check on the format and data being written to the race weblinks tables.

- Consider how the tables might be amalgamated. Not easy given their heterogeneous nature. 

- Look for Coursera (or MOOC) MySQL database course


------------------------------
Notes from Mon 06 Mar

I've had a reasonable session working through the race results functions and converting them across to writing to the database. Unfortunately I've been reminded the race result tables don't kick in until at after 2007. I was able to successfully update 'GetRaceWebLinks' function from the '02_function_....' file. 

Next steps

- Continue working through race results functions. Specifically, I'm up to the writing of tables in the '03_function_race_results_table.R' which is called out as the 'write_race_results_tables' function in '170211 - Test script for running race result' function.

- Sort out management of open database connections. This is throwing up an error in the '03_function_race_results_table.R' file. 

- Consider how the tables might be amalgamated. Not easy given their heterogeneous nature. 

- Look for Coursera (or MOOC) MySQL database course

------------------------------
Notes from Mon 06 Mar

I finally sorted out the 'rider_master_list' and was able to retrieve all 1572 rows of rider names from the database. The other issue is that I've imported only the riders form the UCI WouldTour teams. I probably also need to import the riders from at least the UCI Pro Continental teams as well.

I couldn't identify any sort of loading error WRT the 'Error : failed to load HTTP resource' that came up yesterday. 

Team categories on the Cycling News website
2005: Four tables: UCI WorldTour (20 teams), Teams (3 teams), UCI Pro Continental Men's (29 teams), UCI Continental Men's (122 teams)
2006: Four tables: UCI WorldTour (18 teams), Teams, (13 teams), UCI Pro Continental Men's (30 teams), UCI Continental Men's (lots)
2007, 2008 & 2009: as above
2010:2017, : Three tables:  UCI WorldTour, UCI Pro Continental Men's & UCI Continental Men's

Next steps

- Work out how to capture team history

- Work through completing the download (and writing to database) of all the race results.


------------------------------
Notes from Sun 05 Mar

I sorted out the issue with team names. The problem was simply related to not having the removeDiscritics function running over the team names. (It was doing riding names only).

I also fixed the riderMasterList error. Again a simple issue. I had the title of the function in the wrong spot. I needed to 'assign' the function to the riderMasterList title.

I've successfully run the entire rider list webscrape, however I got the following error message in what I think is the run of 2016 team tables:

I have done 1 of 18 - gonna sleep 0.24 seconds.
Error : failed to load HTTP resource

In addition: There were 28 warnings (use warnings() to see them)
I have done 2 of 18 - gonna sleep 0.98 seconds.
Error : failed to load HTTP resource

I have done 3 of 18 - gonna sleep 0.85 seconds.

So there's a chance that teams 2 and 3 of 2016 weren't extracted.


Next steps

- Check the error above.

- Work out how to capture team history

- Learn more database skills. e.g. Assignment of PRIMARY KEY. I've got a copy of a SQL database for beginners book that's now in my Calibre library.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.


------------------------------
Notes from Sat 04 Mar 

I believe I've sorted out the function to extract rider details from the CN website. The issue appeared to be the assignment of the data/value to the table to be written in dbWriteTable.

I've also nearly sorted out the Master Rider list, although it's giving me a funny error relating to the LPAREN "}" in my function 'riderMasterList'.

I've just set the getRiderList function to run, extracting all of the rider data from every team from 2005 through to 2017. This should hopefully load all of them into the ProCycling database, with correct table and column names.

Next steps

- Fix 'riderMasterList' function error.

- Work out how to capture team history

- Learn more database skills. e.g. Assignment of PRIMARY KEY. I've got a copy of a SQL database for beginners book that's now in my Calibre library.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.








------------------------------
Notes FROM Thu 02 Mar (written at the end of my morning session, Wed 01 Mar)

#### NOTE: I'm changing my notes date system to specify the date on
#### on which I've written the notes, instead of the date I'm next
#### going to use them. 

I've been dealing with another non UTF-8 character issue, this one in the names of riders.
Sadly, converting the encoding to UTF-8 on a string just inserts all sorts of unusable characters. It appears that the fantastic removeDiscritics function is still the main solution.
Today I inserted what I assume is a Scandanavian character for the letter o. I think I'll just have to keep updating this function as I come across problem characters. With luck, I've got 99% of them. 

Next steps

- Sort out why the 2009 rider list is not writing to the database.

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Learn more database skills. e.g. Assignment of PRIMARY KEY. I've got a copy of a SQL database for beginners book that's now in my Calibre library.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.



------------------------------
Notes for Thu 02 Mar (written at the end of my morning session, Wed 01 Mar)

Good progress on the rider_list script. I was able to:
1. Fix up the latin characters (convert to UTF-8) on the rider names [I can see this is going to be an ongoing issue]
2. Write a combined annual list of years to the database
3. Insert a 'sleep' function to insert delays between webscraping rider data for a team

Next steps

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Attempt to sort out assignment of PRIMARY KEY

- Fix non UTF-8 characters in the race results tables. Race name and race location are still being problematic.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.





------------------------------
Notes from my day of coding - Tue 28 Feb

Database Day!

Not as much progress as I had hoped. Finding more problems with unique non UTF-8 characters.

I have managed to sort out the table name and column naming conventions to allow the race calendars to be successfully written to the database. I'm still having problems with assignment of the PRIMARY KEY.




------------------------------
Notes for Thu 02 Mar (written at the end of my morning session, Tue 28 Feb)

I had a more successful time sort out the rider list webscrape today and I think I've got it sorted out. My test runs have been limited to the first few teams in any given year. I had two problems:
1. For some reason I was using the xPath for Nationality when I wanted the UCI ID data; and
2. I needed to find a way to extract just the UCI number. gsub came to the rescue!

I can now generate an overall list of riders for a year. I think it will be useful to put each of these rider lists for each year into the database. I'll still want a master rider list that is a table with only one entry for each rider. 


Next steps

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Once the above master list is created, it should hopefully be straightforward to write this table to the database. The uci.ID for each rider should in theory act as the Primary Key.

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt


------------------------------
Notes for Tue 28 Feb

Woke up late (6am) and have done about 50mins of coding. Didn't really solve my issues with the tables for each team. I'm not extracting the UCI ID cleanly.


------------------------------
Notes for Mon 27 Feb

A decent morning today (Sat 25 Feb). I've managed to finish the script that extracts rider data from the CN website. At the moment I've got a table for each team (in each year) that lists their riding roster, with the following elements: rider.name, rider.link, team.name, dob, nationality & uci.ID

Although I've essentially got all of the data, I now need to organise it into a more useful dataframe that will make later queries much easier. The main trick will be working out how to efficiently deal with all of the rider duplications each year.

It's been gold inserting the Windows Progress Bar. On long (download) scripts it's difficult to know whether it's working it not!

Next steps

- Fix the rider list (for a complete year) component of the script

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Once the above master list is created, it should hopefully be straightforward to write this table to the database. The uci.ID for each rider should in theory act as the Primary Key.


------------------------------
Notes for Sat 25 Feb

Although it didn't feel like I acheived too much from a coding perspective today, I actually made a good amount of ground on the steps required to pull down rider data. It's evident I'm much better at scraping html data now as I can isolate XML nodes and their relevant Values and Attributes pretty quickly. 

After some research, I've decided to use the Cycling News data for rider information. It goes back the year 2000, has pages that follow a logical naming convention and it (importantly) has the UCI ID for each rider (nationality and birthdate combined). 

http://www.cyclingnews.com/teams/2016/
http://www.cyclingnews.com/teams/2016/team-sky/
http://www.cyclingnews.com/riders/peter-kennaugh/

http://www.cyclingnews.com/teams/2000/   # Year
http://www.cyclingnews.com/teams/2000/7up-colorado-cyclist/   # Team
http://www.cyclingnews.com/riders/mike-ley/   # Rider


Next Steps

- Go and get rider information (continue my work on file 170223 - CN Rider Data Initial Script.R)

- Fix database table name error.

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt


------------------------------
Notes for Thu 23 Feb

I've simplified the format of the table IDs so they might be accepted by the MySQL database"

Calendar table:  C.2016
Event ID:        E.2016.001
Race ID:         E.2016.001.R01
Race Table:      E.2016.001.R01.T01

MySQL table column name formats. Doesn't like '#' and 'Rider Name (Country) Team'. I fixed this up, however I'm still getting the following error:

 Error in .local(conn, statement, ...) : 
  could not run statement: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '.002.R01.T01 
( `row_names` text,
	`Pos` text,
	`RiderName.Country.Team` text,
	' at line 1

I suspect it's to do with the use of "." which I want to force in for the naming of Tables. I've been working with these scripts:

170211 - Test script for running race result function.R
03_function_race_results_tables.R

Next Steps

- Fix database table name error.

- Go and get rider information

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt



------------------------------
Notes for Tue 21 Feb

Replaced script writing race tables to .csv files to database files. First run encountered error relating to max length of table name!
Successfully updated Windows progress bar to include label with percentage progress and correct title.


Next Steps

- Fix database table name error. Will likely require a re-think on the table name (which needs to be a unique race identifier).
Error in .local(conn, statement, ...) : 
  could not run statement: Identifier name 'R2015MarsCyclingAustraliaRoadNationalChampionships_01_TbNo_01_Result' is too long

- Go and get rider information

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt


------------------------------
Notes for Mon 20 Feb

I've got the full 'GetAllRacesInAYear' function going, which is a huge achievement! Essentially this now allows me to go and extract ALL of the results tables for any of the years captured on the Cycling News website.

I sorted out the 'event.ID' and 'event.name' columns problem by replacing the 'unique' function with !duplicate[n, ]. 

Next steps

- Fix the use of unallowed characters in the write.csv function. A results table with the '*' at the end (as a note) caused the write.csv function to fail.

- Make use of the 'label' component of the winProgressBar function
https://www.r-bloggers.com/progress-bars-in-r-using-winprogressbar/

- Definitely need to progress to writing to the database instead of thousdands of .csv files.



------------------------------
Notes for Sat 18 Feb

A productive morning. I wrote a functions for the race results weblinks in my TDF Predict project and a function to extract all of the Velogames stage results tables for the TDF.

02_function_obtain_event_race_results_weblinks.R
170130 - Velogames Results Table Scrape.R


I'm having a minor amount of problems with my 'GetRaceWebLinks' function. The 'event.ID' and 'event.name' columns are returning the number '3' for every result!

- Continue writing code to go from start to finish. 



------------------------------
Notes from my day of coding - Wed 15 Feb

A big day, and some big wins. The main disappointment is not getting further with my database skills. That will require more effort and time.

I managed to fix up the script to deal with variations in the results tables and how they are named. A bunch of IF statements sorted that out. I'm very close to being able to send R off to webscrape a whole year's worth of race result tables and to put them into .csv files with a unique 'race.ID'. 

On the database front, I've mastered the following:
a. Connecting to the MySQL server
b. Creating a database 'ProCycling'
c. Creating calendar tables 'calendar_cn_2009' etc in the database
d. Executing basic queries and putting the results into a dataframe

Next steps

- Continue writing code to go from start to finish. I feel like I've got the race results tables scraping function essentially sorted. I mostly need to complete the global script to join the calendar table to a loop through all of the races.

- More database skills. I'll soon need to write all of the race results tables somewhere and I'd prefer to write them straight into the database. I need to sort out assigning the Primary Key.



------------------------------
Notes for Tue 14 Feb

Continuing to work on the big kahuna - a full script to go from calendar through to results webscrape and putting it all into tables.

I'm having problems running the getCNresults function on randomly picked event tables with results weblinks. Currently practicing on:
http://www.cyclingnews.com//races/fenioux-france-trophy-isgp3/sprint/results

I realised that not all results tables will be the same (different number of columns and other attributes), so I've made some adjustments to account for that. But I'm having problems.

Next steps

- Continue writing code to go from start to finish. It's hard going (given the scale), but it's useful starting to tease out all the challenges with extracting and sorting the data.



------------------------------
Notes for Tue 14 Feb

Although progress seemed a bit slow at first, I was eventually able to successfully take the Cycling News annual race calendar .csv files and acheive a couple of things:
a. Convert the latin based characters in the race.details column to standard ASCII; and
b. Create new columns (with this clean data) for event.name and event.ID

170213 - CN Calendar - Latin to ASCII and create event name and event ID columns.R
170213 - Function Latin to ASCII.R

The latin conversion was acheived with a function shared on the following website:
http://stackoverflow.com/questions/15253954/replace-multiple-arguments-with-gsub

Next steps

- Write a basic script to follow through race result extraction from start (annual calendar) to end (writing race result tables) 

- Start learning database skills. With MySQL installed and connected, I can probably do all of this from R.
http://cse.unl.edu/~sscott/ShowFiles/SQL/CheatSheet/SQLCheatSheet.html
https://blog.udemy.com/sql-queries/



- It might make sense to have all the results links for a single year in one dataframe and one csv file.



------------------------------
Notes for Mon 13 Feb

I successfully updated my function script that extracts the tables of race results from a cycling news race results webpage. I was reminded that a function can only return one object, so it's not possible to return all the tables as separate dataframes. The solution this time is to write a unique .csv file for each race result table. In the near future, I believe I'll be writing these table to a database. 

170118 - cyclingnews webscrape v1.R

I also had to find a way of uniquely naming  each table. I tidied up the table caption (from XML) extraction and gave each .csv file a name that includes a unique race.ID, the table number (in order on the page) and the caption for each table. This data should in theory all be important when I go to sort out the database files.

Which brings me to some of the unique fields in the database I think will be used as "KEY" fields - important in relational database like SQL. These are fields I think will be important. 

event.ID   # An ID for the cycling event. Must be unique. e.g. TDF2016
race.ID    # This could be a stage, or just a single race  e.g. TDF16S1
rider.ID   # Each rider needs a unique shortform ID. I think the UCI uses Nationality and birthday   e.g.  14031977.AUS
team.ID    #  e.g. Astana 2016 - AST2017


In an extra - I managed to quickly install MySQL and set up a server - and then connect from R, creating a table!

170211 - Sandpit scripts for creating and accessing MySQL database.R


Next steps

- Write a basic script to follow through race result extraction from start (annual calendar) to end (writing race result tables) 

- Start learning database skills. With MySQL installed and connected, I can probably do all of this from R.

- It might make sense to have all the results links for a single year in one dataframe and one csv file.

- Need to work out unique IDs for tables. Race ID, Rider ID, Team ID, etc so that database elements can be called and linked correctly.


------------------------------
Notes for Sat 11 Feb

Success! I was able to isolate the results link infomation, put it in a dataframe, get rid of duplicates. I then extended this to a loop to run through all of the events in the 2016 calendar. I managed to have a csv file written for each race with the results weblinks.

170207 - cyclingnews race webscrape v1.R

Next steps

- It might make sense to have all the results links for a single year in one dataframe and one csv file.

- Need to work out unique IDs for tables. Race ID, Rider ID, Team ID, etc so that database elements can be called and linked correctly.




------------------------------
Notes for Thu 9 Feb

I had my first go at scraping data from an individual race webpage. I used the first race at the top of the 2016 calendar (scraped from CN). I can isolate the XML attribute with links to race results, but isolating the link info by itself and narrowing to just the right race links is proving a bit of a challenge.

170207 - cyclingnews race webscrape v1.R

Next steps

- Continue to refine code to scrape the weblinks for race results pages

- Modify Calendar webscrape to include a dataframe column for the raw 'race' weblink. This looks like it will be important on subsequent weblink searches. i.e. the link without "www.cyclingnews.com" at the front.

- Start thinking about what my main R functions will be and how they will call each other. [Practicing funtions at work might be useful]



------------------------------
Notes for Tue 7 Feb

I spent the entire morning attempting to set up a working GitHub account linked to RStudio. It appears to be working properly. I was able to 'pull' my old ProjectAssignment2 files (Coursera Data Analysis course) successfully down to RStudio. I've unfortunately got plenty to learn about version control and using GitHub with R, but at least I now have a basic backup and version control of my working files. 

Next steps

- Investigate potential free & private Git services (GitHub is $7/mth for private)

- Look at the next step - going to the web links and determining the pathway to race results.

- Download/Install the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.



------------------------------
Notes for Mon 6 Feb

Still going well. Today I successully created 
   # the 'Start.date' and 'End.date' columns in the calendar_cn data.frame
   # Wrote the results of each data frame to a .csv file

I had a good time learning about regular text/string expressions such as 'regexpr', 'nchar' and 'substr'.
I used these to split up the 'Date' field into start and end dates.

So I now have .csv files for all of the years between 2005 and 2016 with full calendar informaiton, including weblinks to each race and start and end date columns.

Next steps:

- Look at the next step - going to the web links and determining the pathway to race results.

- Sort out a Github account and start some version control

- Download/Instal the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.


------------------------------
Notes for Sat 4 Feb

Great news! I was able to subset out the weblink for each UCI calendar entry and correctly enter it into my dataframe in the right row. My code is able to successfully ignore this action for rows with no web link.

Next steps:

- Write in loop for the new code to cycle it through all the Cycling News calendar years (easy)

- Adding additional columns to the calendar tables
	* Instead of editing the Date column, create a 'Start' and 'End' column

- Look at the next step - going to the web links and determining the pathway to race results.

- Sort out a Github account and start some version control

- Download/Instal the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.



------------------------------
Notes for Thu 2 Feb

Still not having a great deal of luck creating a table with all of the calendar informaiton (weblinks and race data). I did have a bit of success using xpathApply to extract a row element of XML data, however I'm unable to then use this subset to extract the needed information. It's great that I've got a subset of XML data with each row that has all the information I need (in either the xmlValue or Attr data), but it's frustrating that I can't access this yet.

170124 - cyclingnews calendar webscrape v2 - uses xpathApply.R

Next steps:

- Continuation of how to subset xml data to collect race data and web link


------------------------------
Notes for Tue 31 Jan

I didn't have a great deal of luck connecting up the weblinks for races and the complete table for the race calendar. However I did manage to learn some more about extracting XML data using xpathApply. For the first time I was able to extract a neat table from the Velogames results webpages.

I'm still using this to find a neat way of extracting the table information from Cycling News webpages, including the html links.
I think I'm going to have to use a more manual extraction that takes all the data and then combines it into a table in the manner I want.

I also discovered that the next webpages have stage data in a somewhat inconsistent manner than may challenge my approach to automatically generate links to results pages.


Next Steps:  Largely still what is mentioned below for Mon 30 Jan.


------------------------------
Notes for Mon 30 Jan

I was able to extract the web link information for each race from the calendar html data.
The bad news is that not every race in the table has its own page and therefore a link is missing for some race entries.
This means the length of the web link list is different to the number of rows in the table and they don't match.
I'm therefore currently looking at ways to extract the web link data at the same time the table is read into R.

Next steps

- Finding the best way to extract the web links for each race and add it to the calendar data frame.

- Sort out a Github account and start some version control

- Download/Instal the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.


------------------------------
Notes for Sat 28 Jan

Good news! I've written the code to go and get the UCI cycling race table from Cycling News for year 2005 through 2016 (or whatever years I want).
I also inserted code to cleanup the Date column (removing all the uncessary text). 

Next Steps

- Adding additional columns to the calendar tables
	* Instead of editing the Date column, create a 'Start' and 'End' column
	* Add a column for the (anticipated) race results web page
		Two ways to do this: a. educated guess; and b. draw link info from html data

- Look at what other race details would be useful (distance, weather, elevation, altitude, etc)


Note: There is text in the race table that has details of the web address for the results

http://www.cyclingnews.com/races/santos-tour-down-under-2010/stages/
http://www.cyclingnews.com/races/santos-tour-down-under-2010/stage-6/results/


------------------------------
Notes for Wed 25 Jan

I successfully wrote a FOR loop that extracts the tables from the calendar of road races for a particular year on the Cycling News website. These go back to 2005.

Next steps

- Cleaning up the calendar table
	* Multi-day races have messy Date entries
	* Some events have year unique names e.g. - 94th Gent-Weldveren
- Writing a loop that extracts the same table for years 2005 through 2017
- Going to the next level and getting the race weblink for each race


------------------------------
Notes for Tue 24JAN17

I've successfully written a FOR loop that extracts the results tables from a Cycling News race results web page (at least for the Tour Down Under 2016). 


Next steps

- Cleaning up the tables
	* Separating Rider/Country/Team
	* Filling in empty finishing times

- A webscrape that pulls the World Pro Tour race schedule for the year (2016)
	* Race names and attributes (# and names of stages, distances, elevation, date, location, start/finish, weather)
	* Identifies Cycling News naming convention for races
	* http://www.cyclingnews.com/races/calendar/2016/








# Webpages for Cycling news

Santos Tour Down Under
http://www.cyclingnews.com/races/santos-tour-down-under-2016/

Stages
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stages/

Down Under Classic (Adelaide)
http://www.cyclingnews.com/races/santos-tour-down-under-2016/down-under-classic-adelaide/results/
Stage 1
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-1/results/
Stage 2
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-2/results/
Stage 3
http://www.cyclingnews.com/races/santos-tour-down-under-2011/stage-3/results/
Stage 4
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-4/results/
Stage 5
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-5/results/
Stage 6
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-6/results/


Gent - Wevelgem
http://www.cyclingnews.com/races/gent-wevelgem-2016/
Results
http://www.cyclingnews.com/races/gent-wevelgem-2016/results/

Paris-Roubaix
http://www.cyclingnews.com/races/paris-roubaix-2016/
Results

Giro d'Italia
http://www.cyclingnews.com/giro-ditalia/
Stages
http://www.cyclingnews.com/giro-ditalia/stages/
Stage 1
http://www.cyclingnews.com/giro-ditalia/stage-1/results/
Stage 2
http://www.cyclingnews.com/giro-ditalia/stage-2/results/
Stage 20
http://www.cyclingnews.com/giro-ditalia/stage-20/results/
Stage 21
http://www.cyclingnews.com/giro-ditalia/stage-21/results/


http://www.cyclingnews.com/races/giro-ditalia-2011/

http://www.cyclingnews.com/races/giro-ditalia-2011/stages/

http://www.cyclingnews.com/races/giro-ditalia-2011/stage-1/results/

http://www.cyclingnews.com/races/giro-ditalia-2011/stage-2/results/

http://www.cyclingnews.com/races/giro-ditalia-2011/stage-21/results/

