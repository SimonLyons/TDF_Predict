

------------------------------
Notes from Sat 28 Oct

I'm making some progress on my script to evaluate lists of riders of different lengths. I've got a table () with the matching data including the max Levenshtein results. From this I can locate the duplicates and work out which one is the best match and which one(s) should be removed as matches. I've also got some preliminary script exploring how to identify duplicate matches and work out which one is the best match.

One of the unexpected benefits of pulling this data out is that it shows a lower than 1.00 Levenshtein result for names that are a match, where the names have differing number of words (e.g. Mikel Landa Meana and Mikel Landa). This instantly makes me think I need to return the mean of only the number of words that match the length of the shorter name. This should firm up the results and possibly help avoid matching errors.

At this stage I can think of two methods for identifying (and therefore removing) incorrect rider matches:
1. Finding duplicates in the returned matching list and only keeping the best match; and
2. Only retaining matched names above a specified minimum calculated Levenshtein value.

The other method I have in the back of my head with respect to a better match is somehow attempting to mimic how we (humans) assess name matches. Things we might take into account could include:
a. Giving less weight to christian names and more weight to surnames
b. Considering the order of names. e.g. Ignoring a match between 'David' as a first name and 'David' as a surname.
c. Recognising middle names
d. Only taking into account maximum number of words in the shortest name.

Next steps:

- Continue work on differing list length script
    * Take 'name_match_table' and start refining/removing duplicates
    * Built a working script into a generic function

- Improve current name matching function to take mean of only the number of words from the shortest name.

- Look at running a test with limited matching names.

- Insert list cleansing functions such as replacing special characters and converting lists to correct vector type

   
------------------------------
Notes from Thu 25 Oct @work

I've done a bit of work commencing how I might approach dealing with lists of matching lengths. I've changed the main wrapping Levenshtein function so it outputs all of the calculated Levenshtein values for each name so they can be used to evaluate duplicates. 

Next steps:

- Continue work on dealing with mismatched lists. Look for ways to use all Levenshtein values to pick the best match for a name from the shorter list.


------------------------------
Notes from Thu 25 Oct

Okay - testing is going okay. What happens when the string vectors (for the two lists of rider names) are of different lengths?

I've run a test between the 2017 TDF starting list of riders (198) and the riders finishing Stage 11 (180). 

It's evident there are duplicated riders - as a result of having less riders finish Stage 11 than there were riders starting the TdF. Not surprising really. So we have duplicated results because my function finds the best match for each of the starting riders. 

I think the method for matching only the correct riders will be to exclude the duplicates. I'll need to retain the calculation for the Levenshtein distance for each rider match and (in the case of the duplicates) use the best match to exclude the other duplicate results. I'm not sure how to approach this yet....

Next steps:

- Find a method for excluding missing riders. 
 

------------------------------
Notes from Tue 24 Oct

I improved the fuzzy name matching script by replacing the Soundex conversion to a lowercase ('tolower') conversion and leaving the straight Levenshtein distance calculation. This fixed up the mismatch with Andrey Amador Bikkazakova. More testing is required.

I've now created a discrete function script called 'FuzzyNameMatch' stored in the following file.

12_FuzzyNameMatch_function.R

Next steps:

- Finish off the neat FuzzyNameMatch function

- Conduct testing of the new function.

- Consider whether it should be converted to return the position of the best match, so it might be used more effectively in merging dataframes.


------------------------------
Notes from Mon 23 Oct

I'm making good progress with my improved fuzzy rider name matching function. I've just run it and I'm only getting one incorrect match:

22 Andrey Amador Bikkazakova Crc Andriy Grivko

The next test will be to modify if with the soundex conversion removed. I have the feeling this will be better now we're testing each individual word, and we've making accommodation for names of matching length.

Next steps:

- Modify new levenNameAgainstNameList function to remove soundex conversion and re-test.
- Perhaps consider extending function to run both soundex and non-soundex matching and compare results, taking the best one.


------------------------------
Notes from Sat 21 Oct

I'm attempting to build the function allowing more refined Levenshtein Sim matching by isolating names pairs where the number of words match. I think this comparison needs to occur in the (nested) levenFullNameList function and below I've currently got it in the equivalent of the final levenNameAgainstNameList function.

The kids are awake so I'm finding it hard to concentrate. Monday!

Next steps:

- Re-jig my work with the IF/ELSE statements so they are used in the name/name function


------------------------------
Notes from Thu 19 Oct

I've had some success finally combining IF statements with sapply. It works for the basic function I've created to test it out. Next I need to extend this to doing a variety of things with fuzzy string matching against a list of riders.

Next steps:

- Extend IF statement function to sapply of list of riders. Aim is to build some weighting or ordered Levenshtein matching.


------------------------------
Notes from Wed 18 Oct

More work on fuzzy matching practice file.


------------------------------
Notes from Tue 17 Oct

I'm making a bit of progress on optimising my fuzzy search function. It looks like it's possible to introduce IF statements into 'apply' functions:

https://stackoverflow.com/questions/14196696/sapply-with-custom-function-series-of-if-statements

I'm looking to optimise my fuzzy matching by enhancing the Levenshtein calculation where name length of search term and name from the list are the same. This will allow me to do the Levenshtein calc in order of words, not simply on every word and pick the best match.

Next steps:

- Finish first attempt at IF statement looking to match the number of names.

- Extend optimising IF statements. Possibly looking to weight surnames.


------------------------------
Notes from Mon 16 Oct @work

I didn't get very far extending my testing.


------------------------------
Notes from Mon 16 Oct

Just a bit of quick testing against my new function. I extended the test to the full list of riders starting and on Stage 1 of the 2016 TdF. Strangely, I get a 100% match using the ClosestMatch2 function and using my new levenFullNameList function I get a miss on one name:
"Andrey Amador Bikkazakova" returns "Andriy Grivko"

Anyway - I need to do a bit more testing to understand what is going on.

The function appeared to run relatively slowly too (compared to the other matchign functions which were almost instantaneous). 

Next steps:

- Further testing on my levenFullNameList function to see what is going on.


------------------------------
Notes from Sun 15 Oct

So great news! It looks like I've got my own fuzzy matching function working properly and initial testing results are looking really good. It's already correctly matching names the standard fuzzy matching techniques were not getting.

The problem with my nested sapply functions was as simple as having the input variable (search name) and the list (list of rider names) around the wrong way, so a list was being fed to a variable, instead of the other way around. Good to have that sorted.

I have located one non-match:  "Andriy Grivko" returns "Andrey Amador Bikkazakova "

There could be a number of reasons for this. Andriy GRivko could be missing from the list of rider names on Stage 1 and Andrey Amador could simply be the best match.

Next steps:

- Testing on my function against the TdF rider lists. Expand to run matching against everyname in both lists.

- Look at development of my matching function so it returns the most useful values. A cracking result would be a dataframe with the input name, the return matching name, the position of the matched name in the list and finally the Levenshtein value.


------------------------------
Notes from Sat 14 Oct

I'm making some good progress with lapply and sapply. I finally worked out how to use them instead of a loop. I think it's neater. When replacing a basic loop it doesn't seem much cleaner, but I guess it becomes magnitudes cleaner when the loops and function are more complex.

I've nested some sapply() steps to walk through matching the soundex values for a search rider name against a list of rider names. It was working until the final step, which extends the matching to the entire rider list.

Next steps:

- Sort out the error with my final sapply step. I think it's feeding an incorrect class (list) back into an earlier function, which I think is in my levenNameList() function.


------------------------------
Notes from Thu 12 Oct

I'm up to this bit here. I've got some script which does the soundex() matching for the search name against the first name in the search list. I like the method I've employed. If the number of words match, I simply perform the Levenshtein Sim calculation on each of the soundex words in order. 

If the number of words don't match, then I throw the whole book of matching, calculating the Levenshtein Sim result for each word in the search name against every word in the first name of the search list and then picking the top results (reducing it to the length of the shortest name).

Next steps:

- Extend the script I've written for matching just the first name in a list of names, out to all of the names in the list. Man it would be nice to do this without using a FOR loop, but I can't see how this would be acheived with lapply or sapply. I guess I'd need to write a function and feed that to lapply.....


------------------------------
Notes from Tue 10 Oct

I've done a small amount of work on improving fuzzy string matching. I've come across a function developed by Robin Edwards (geotheory) on GitHub called 'bool_detect'.


------------------------------
Notes from Mon 09 Oct @ work

I've done a really quick amount of work starting the process of fuzzy string matching with both the search term (a rider's name) and the search string (a list of rider's names) into individual words. I'm planning on coming up with my own weighted fuzzy matching script that give more emphasis to names other than the rider's first name (which are far more common). 

Next steps:

- Continue split string search fuzzy matching in Fuzzy Matching Practice file.


------------------------------
Notes from Sat 07 Oct

Okay - so I've finally progressed through my Age variable builder. The not so great news is that due to challenges with fuzzy string matching (and also just riders missing from the rider master table) I've got 20 of my 174 2016 TdF riders missing.

Anyway - I've been able to work out the Age of each rider at the start of the 2016 Tdf (using the date of birth data) and do a quick plot of finishing position against age. There isn't any clear relationship, either with the full set of riders or a subset of the top 20 finishing positions.

My work on understanding an improving fuzzy string matching has been excellent though. I need to do a bit more on this, but in terms of retrieving data from the ProCycling database and matching strings within R. 

Next steps:

- Further work on more sophisticated fuzzy string matching.
    ~ Write script to test accuracy of matching results (particularly for database)
    ~ Look at how name matching might be improved with weightings (e.g. on surname)
    ~ Look at why MySQL boolean matching might be superior


------------------------------
Notes from Thu 05 Oct @ work

I've added some of the script I created to address the fuzzy matching problem posted on the forum. I've done a little bit using the Levenshtein algorithm and also a short bit combining both Levenshtein and Soundex functions. 

The additions are at the bottom of the following file:

171003 - Fuzzy Matching Practice.R


------------------------------
Notes from Thu 05 Oct

Okay, so I'm making progress, but still coming across problems. The ClosestMatch3 function I've created is working, but the levenshtein distance calculation doens't necessarily return the correct match. The example I've checked is "Daniel Moreno", which returns "Daniel Martin" instead. This must be due to 'Dan Martin' returning a better levenshtein distance than the full name in the rider list table which is 'Daniel Moreno Fernandez'. I'm not sure what will solve this problem!

The second, less troubling issue is there are riders in who finished the 2016 TdF who aren't in my database rider table. I think the main reason is these riders aren't in the ProTeams. I need to do more checking. 

Next steps:

- Continue work on fuzzy matching. Work on above issues. 


------------------------------
Notes from Tue 03 Oct @ work

I'm actually having some success today sorting out a solution for fuzzy string matching. The best approach appears to be using a Levenshtein Distances function to identify the best match. I haven't found a native function that returns the best result, but a small function script on the web using the 'RecordLinkage' calculated the Levenshtein distance and then picks the biggest value to return the best match. I've then extended on this function to allow quick application of matching a character vector (list) from one dataframe to another character vector (list). I've stored the function I'll hopefully be using a lot in the following script file:

10_ClosestMatch3_function.R

I've inserted some new script into my Age variable builder using the new CloseMatch3 function.

Next steps:

- Trial new matching function in Age variable builder script.


------------------------------
Notes from Tue 03 Oct

I'm making progress on my fuzzy matching. Although the SOUNDEX approach was good, it was missing out on names like Alejandro Valverde because he is listed with an additional name (Belmonte) in the rider_list_master database table. 

I've been experimenting with the MySQL functions MATCH / AGAINST which seem to work quite well when combined with BOOLEAN operation. I'm still missing riders, so it requires more investigation.

Additionally, when I merge my dob table with my results table, another perfect matching activity is going on and not all riders are being matched up. I'm missing 14 riders in my dob dataframe, but I'm missing 35 from the merged results/dob table.

Next steps:

- Continue work on fuzzy matching. 


------------------------------
Notes from Mon 02 Oct

I'm working on the script to build the Age variable. The code I started in Broadbeach yesterday (without access to the database) was very useful and only need minor modification once I found the correct name of the rider list table (rider_list_master).

I've pulled the relevant DOB data for the riders who completed the 2016 TdF. The initial issue was incorrect DOB data, which I corrected using the UCI ID for each rider (which conveniently contains each rider's correct DOB). The next issue is it was immediately obvious I had not extracted all riders. 

So I'm currently working on how to correctly match rider names qith queries in the ProCycling database. I knew this problem was coming, right from the start. The variety of ways names are defined means perfect matching would never work. The first name to stick out on my current work is Alejandro Valverde who is listed as 'Alejandro Valverde Belmonte' in the Cycling News rider info page and therefore has this longer version of his name in my rider_list_master database table.

I've been using SOUNDEX for approximate fuzzy matching, but it looks like this is useful when the string length is almost the same. SOUNDEX appears to deal well with incorrect or differing characters but not searches where there might be additional text (such as riders with extra names.....)

I've now got the FULLTEXT searching functions working. I need to ALTER the rider_name column in the database to be a FULLTEXT column. So the MATCH / AGAINST functions are now working, however my query is now returning all rows with similar matches. For example, Christopher Froome is now returning 5 rows of riders with Chris in their names.

Next steps:

- Seek to understand how to limit the results of FULLTEXT searches. Need to return the best result.




------------------------------
Notes from Sun 01 Oct @ Broadbeach

I've updated the FP vs team mates pairs visualisation to show teams by colours. There are 22 teams, so the variation isn't always obvious. Anyway - nothing new jumped out in terms of a relationship between finishing position and the number of team mates completing a tour in a particular previous year. 

I guess you could add up all of the completed tours for each rider and tally up this for each team to get an overall experience level for the team. 

I've started building script to extract the Age for each rider completing the 2016 TdF. It's in the file: 171001 - Variable builder - Age.R

Next steps:

- Continue work on variable builder for rider Age


------------------------------
Notes from Sat 30 Sep @ Broadbeach

So I've just completed an initial visualisation and regression analysis of the dataset I created with variable sets for the number of team mates (in 2016) that have completed the TdF in each of the previous years. The visualisation doesn't reveal any meaningful relationships and the regression analysis therefore returns a model of predictably (no pun intended) low accuracy (RSquared ~ 0.02! i.e. 2%)

Although the results aren't fantastic, the outcomes are reasonably intuitive. It's a bit like the predicting a rider's finishing position based on the mean of his team's finishing position in a previous year. It's obviously not going to reflect the individual results of each rider. 

However there may be a broad influencing factor. Certainly the team each rider is on has an impact on their performance. So I think I'm going to take another look at the visualisation with colours for each team. My gut instinct tells me the experience of a rider's team is going to make a difference. Of course a rider may be very experienced without having ever completed a previous TdF. They may have ridden a great number of the other Grand Tours, or other races that would help. 

Next steps:

- Colour code by team the team mates visulation.

- Look for next set of variables. Maybe Age?

- Perhaps do regression analysis with small set of what I think will be influential predictive variables: Previous finishing position, team mates experience


------------------------------
Notes from Fri 29 Sep @ Broadbeach

So I've combined all of the years via the one-at-at-time merge() process. It's produced the number of team mates in each year I was after. I've had a quick look at the plot of pairs and there doens't appear to be any significant relationship between the finishing position in the current year and the number of team mates that rider has who have completed the TdF in previous editions.

Next steps:

-I need to update the resulting dataframe to replace the NAs in the team columns with zero (0).


------------------------------
Notes from Thu 28 Sep @ Broadbeach

I'm making a bit of slow progress on the generation of the team mates variable set for each year. I'm honing in on the calculation, but haven't yet found an elegant method for doing it for each year in the dataframe in one go.

Next steps:

- Continue working on script to calculate the number of team mates who have completed the TdF in previous years.


------------------------------
Notes from Wed 27 Sep

I'm working on building a count of the number of team mates each rider has that have completed the TdF in previous years. I'm sneaking up on it slowly. Unfortunately I'm having problems mutating a new column with results of the list. Using the rider (in each row) as a filter is apparently challenging.

Next steps:

- Continue working on script to calculate the number of team mates who have completed the TdF in previous years.


------------------------------
Notes from Tue 26 Sep

I'm working on script to determine the number of the riders' 2016 team mates that have completed the TdF in each previoud year. I've built the code that determines this for a selected rider. I'm now attempted to create a sophisticated mutate() action allowing this calculation for all of the 2016 riders and hopefully for years 2015 back to 2011.

At the moment I'm thinking of using the dplyr mutate function to create new columns. I may need to finally start looking at understanding the lapply function. My filtering script may otherwise be far too complex and I want to avoid running a loop if possible.

Next steps:

- Continue working on script to calculate the number of team mates who have completed the TdF in previous years.
 

------------------------------
Notes from Mon 25 Sep

I neglected to include my notes on Simon Gerrans data visualisation from my notepad whilst at Kingscliff.

- No. of races
- No. of minutes/hours
- No. of top ten finishes
- No. of podium finishes
- Decending list of races most commonly entered
- Which races are most commonly top ten and podium'd
- No. of race wins and which races

I've done a bit of minor manipulation on the Variable Builder file, reading in the 'data_table' file which contains the full dataset of results for riders who did the TdF in 2016 and prior, for all races prior to the TdF start in each year.


------------------------------
Notes from Sun 24 Sep

So I've had my first scare with this laptop. It didn't boot up properly and I needed to learn how to do a safe restore. I'm now working on sorting out a backup.

I haven't been completely reckless. All of my code is safely stored on GitHub, and I can continue working on that from multiple computers. But it would be a pain if I lost the setup on the laptop and I need to sort out having a quick recovery service.

I've been working with the Simon Gerrans' dataset. I've built tables showing a list of races by year and then a table with the number of unique races in each year. I'm not working on a table that shows each unique race (rows) and when Simon has races it in a particular year (columns). I'm halfway there.

Next steps:

- Keep working on the Simon Gerrans' dataset. Need to sort out the latest table so there is a single row for each race.


------------------------------
Notes from Sat 23 Sep @ Kingscliff

Very little progress this week, thanks to a busy time on my course, early morning riding and then Liz and the kids arriving here in Kingscliff. I'm motivated to keep moving and I'll work on some small things while Liz is out for a walk and the kids are watching some TV.

I've been working on the variable builder script:
170913 - Variable builder - No of miles and minutes ridden.R

It's evident from my initial results that it's unlikely there's a correlation between the number of hours ridden and TdF finishing position, but it's also a bit early to tell. I've looked at the entire peloton and just the top 20 finishers and the results look pretty erratic. Liz has suggested looking at cumulative riding hours for a number of years, which is a good idea. Anyway, more work to do.

I'm going to include some notes here I jotted down during my course this week. They are suggestions for variable categories to examine:

- Place/country of residence
- Nationality
- Which races
- Hours of racing in the months immediately prior to the TdF (taper may be important)
- Age
- Career length
- Finishing position in particular races
- #/% of same team mates from last year
- # of TdF managed by team directeur
- # of TdF ridden by team directeur
- Similar data as above for Grand Tours

I additionally wrote a list of future TdF Predict project jobs:
- Updating dataset to include latest races
- A semi-automated race update script
- Some auditing and checking: duplicates, empty, corrupt, name conflicts
- Metrics:
   * No. of races each year
   * No./% race categories
   * No. of teams
   * No. of countries
   * No. of riders

Next steps:

- More work on rider duration variables

- Take the Simon Gerrans data and look at ways for visualising this. I think I'm looking for annual comparisons. Stuff like total distance ridden, number of races, number of top ten and podium places.

- Look to build new variables (more MySQL queries required)


------------------------------
Notes from Wed 20 Sep

A very mild start on converting the duration data back into Long format.


------------------------------
Notes from Mon 18 Sep

I'm preparing a few things to work on while I'm in Kingscliff for the week on Boeing's Supplier Management University course. Without access to my ProCycling database, I need to take any data I need away with me. 

It's evident how much I learnt from the MySQL Coursera MOOC. I'm coming back into setting up queries after a decent break (I spent well over a month on Velogames work and doing some predictive analysis) and it's been good to pick up developing fairly complex queries without a lot of drama. I've definitely needed to Google for help, but I get the concepts.

It looks like I've successfully built a query for all of Simon Gerrans' race data, which included a JOIN across three table MySQL tables. This is excellent as doing JOINs across these three tables (master_results_time, race_weblinks_master & race_calendar_master) is going to be quite common and important for my future predictive analysis

Next steps:

- Take the Simon Gerrans data and look at ways for visualising this. I think I'm looking for annual comparisons. Stuff like total distance ridden, number of races, number of top ten and podium places.

- Use the rider duration data in the Variable Builder script and start making sense of that. Firstly I need to convert the combined datatable back into a long format for use in ggplot.


------------------------------
Notes from Sat 16 Sep

I've been working on visualisation associated with my new rider total annual riding time data. I did a plot of all the riders and their annual riding time. The result is a crazy graph that is more artwork than useful plot! Anyway - it's good to get going.

I've created a dataframe combining the TdF finishing position data with the annual riding duration. The next step will be to have a look at a plot with 2016 finishing position against 2016 total riding duration for each rider. 


------------------------------
Notes from Fri 15 Sep - back at home

Oh yeah baby! This is getting fun! The scripts I wrote at work to combine the pre-TDF date range and the rider list into a single MySQL query worked. I only needed to complete the final action of sticking them into the dbGetQuery command. The query took a minute or so to execute, but looks like it's worked. Hooray! The "date_table.csv" file is 10MB, which isn't too surprising given it has over 80k rows of results information!

I've then extended on this by using dplyr to start building some summarising data. I filtered down to just rows containg 'Results' or 'results', we don't want the 'General Classification', 'Young Rider' or other classifications. 

This work is contained in:
170913 - Variable builder - No of miles and minutes ridden.R

Next steps:

- I guess the main thing is to explore this data (perhaps visually) and see if it leads to any insights, and to check if it looks correct (at a macro level). 

- Next I'll scatterplot the results against TdF results to look for any obvious relationships and consider whether they might be useful predictor variables.


------------------------------
Notes from Fri 15 Sep @ work

I've built a combined MySQL criteria for both the pre-TDF date range and the rider list, with a MySQL query directly following it. Now it just needs testing. The date range hasn't been limited to just 2010 to 2017, so this could potentially be chopped.

I've additionally started a new script working on summarising rider data, in this case driven by the desire to look at the performance of Simon Gerrans. The file is called: 170915 - Simon Gerrans Comparo.R

Next steps:

- Test out new combined SUPER QUERY @home

- Continue work on Simon Gerrans summarising comparo file. I'm up to building the criteria for the MySQL filter query.


------------------------------
Notes from Fri 15 Sep

My queries written at work (with a few corrections) looking for TdF dates worked!

Now just looking to feed this information and combine it with the secondary date range filter query.


------------------------------
Notes from Thu 14 Sep @ work

I've created some code to get the ball rolling on searching for the start dates for the Tour de France in each year between 2010 and 2017. This is a good skill area as I'm learning about fuzzy string matching. I'm concerned the Cyling News results will have varying titles for the Tour de France (stored in the race_details column) which will invalidate a simple WHERE filter search looking for 'Tour de France'. Having a good fuzzy string matching search will be critical to my future work, particularly when dealing with the variety in spelling of rider and team names. My work is in the following file:

170913 - Variable builder - No of miles and minutes ridden.R

I've come across a MySQL function called SOUNDEX() which appears to support fuzzy string matching. 

It also looks like 'GROUP BY' and 'MIN' MySQL functions will be very usefull in executing my search for the first TdF date in each year. GROUP BY will be used to create groups for each year (using the YEAR) function. MIN will be used to pick the earliest date (for each year).

Next steps:

- Have a crack at running the code I've developed at work and modifying/fixing as needed.

- Combine the rider filter and date range filter MySQL queries!


------------------------------
Notes from Thu 14 Sep

I was able to run the script I'd written on the bus to work yesterday aimed at creating a new full copy of the master_results_time table and correcting the date format to support full date-based queries via MySQL.

After some testing, I carried out the same action for the master_results_points but it appears as though the modification might not be required as filtering WHERE queries on dates seems to be working on all the original tables....

I've next created the date range query for the date ranges in each year prior to the start of the TdF. I've used dummy dates, but got a range query working for 2016 to 2010. 

Next steps:

- Extract the actual start dates for the TdF between 2017 and 2010 and sub this into my date range builder and query

- Combine the rider filter and date range filter MySQL queries!


------------------------------
Notes from Wed 13 Sep @ work

I did some work on the bus into work and then just a minor amount here at work with the primary outcome being some script to create a full copy of the master_results_time table (named master_results_time_cdf as before for 'corrected date format') and modified to have a correct date field for stage_date.

Next steps:

- Complete fix for dates in master database files so I can efficiently perform date queries. I've written some code at work - I just need to go and execute/test it at home.
My previous date correcting script is at around the 60% mark of the following script:
170705 - Initial Database Analysis.R

- Extend the query to limit results to racing prior to the TdF each year.



------------------------------
Notes from Wed 13 Sep

I'm going to attempt to build the variable for the number of race miles and race minutes each TDF rider has done each year, prior to the TDF. Things I'll need:

* Starting date for the TdF each year. Which give me the idea for measuring the rest they get (from racing) prior to the TdF start.

I think I can just do this in this (to start) with just the race_master_time table from the datbase.

## Right-o: I've had a reasonable amount of success this morning. My work is in the script:
170913 - Variable builder - No of miles and minutes ridden.R

I'm proud of being able to build a big query limiting the results of just the riders completing the 2016 TdF (+170). 

Next steps:

- Complete fix for dates in master database files so I can efficiently perform date queries.
My previous date correcting script is at around the 60% mark of the following script:
170705 - Initial Database Analysis.R
- Extend the query to limit results to racing prior to the TdF each year.


------------------------------
Notes from Tue 12 Sep

I'm at a bit of a minor crossroads trying to work out what to focus on next. 

- Building predictor variables. i.e. pulling the data from my database and collating it ready for analysis
- Updating scraping script to pull latest cycling scripts
- Working through Data Analytics course on EdX to improve my predictive analysis
- Determining new predictor variables. e.g. race miles ridden, race results, etc

------------------------------
Notes from Thu 07 Sep

Yesterday I went back to my TDF data and built up some new variables: Team_Mean, Team_Highest, Country_Mean, Country_Highest. The point was to see whether the introduction of these new variables would improve the accuracy of my basic TDF prediction work.

The effort I put into understanding model accuracy has been valuable as it's allowed me to establish a baseline (with my current predictor model) and then to test the improvements (or otherwise) of modifications and additions. 

The best accurary comparison metric I've found is RSquared, which is a percentage metric. It measures the difference in added value from no-value (where the model predicts no better than the mean of the data) to 100% value (where there is zero error).

For example, it's apparent the addition of Team_Mean and Country_Mean does not add value to the predicitive model. 
FP_2016 ~ GC_Mean + best_GC
'glm' 0.6685979, 'lm' 0.6586792
FP_2016 ~ GC_Mean + best_GC + Team_Mean + Country_Mean
'glm' 0.6100148, 'lm' 0.6288929

It's also useful looking at the scatterplots. The plots for GC Finishing Position in 2016 (FP_2016) against the predictor variables is powerful for spotting a relationship to the predictor variables. The plots for FP_2016 against individual rider GC Mean and GC Highest Position show a clear linear relatioship, which is why these predictors add so much value. It's hard to spot any obvious (at least linear) relationship between FP_2016 and the Team and Country values, which is no doubt why they don't improve the accuaracy of the model.


------------------------------
Notes from Tue 05 Sep @ work

I've extended my work from earlier this morning on determining model accuracy. I built a loop that increases error on my basic linear model and predicts the results using a number of different methods (in this case, 'lm', 'glm' and 'rf'). 

I've introduced error into my y results, increasing it by a factor of 10 three times. By the time the error gets to 1000 the plotted dataset hardly represents the underlying linear model at all, so it's not surprising the prediction models are wildly inaccurate.

The results are stored in an error_df dataframe for comparison. Looking at standardised RMSE is interesting, although predictable (which is good). When the error is low (1), the residual variance is significantly less than 1% for the two linear regression predictive methods. The Random Forest method has nearly 3% variance and is therefore far less accurate. When the error increases to 10 the variances increases to roughly 2% for the linear methods and 3.5% for 'rf'. At error of 100 the linear methods are around 15% in variance and the 'rf' method 17%. Finally the 1000 error value introduces 41%, 42% and 47% variance, which isn't really any sort of accuracy at all. It's interesting to see the 'rf' method perform better (or closer to the linear models) as the error increases.

So the upside is I believe I understand the use of RMSE and RMSED quite well again and I'm ready to start using these measures for testing and selecting prediction methods on my TDF analysis.

Next steps:

- Need to still work out why RMSE calculated in R prediction methods is slightly different to the values I manually calculate (still in R).


------------------------------
Notes from Tue 05 Sep

This morning I've been doing some practice focused on understanding model accuracy better. I've created a dummy dataset (based on a very basic linear model), introduced some error and then set about predicting a model and looking at the error. My work is in the following script:

170905 - Regression Prediction Accuracy Practice.R

I have an understanding of RMSE again, but I'm finding a small difference between the RMSE calculated automatically in R by the model and the RMSE I calculate manually. I'm also additionally seeking to understand the normalised error (I think it's RMSED) to be able to appreciate model accuracy between datasets with different scales.

Next steps:

- Try and work out why my RMSE is different to the one calculated in R. They should really be the same as the formula is straight-forward.

- Seek to understand RMSED better.

- Try out some different prediction methods and compare the accuracy.

- Refine my own method for testing and selecting the most accurate prediction methods.


------------------------------
Notes from Fri 01 Sep

I've enrolled in the EdX course. The reviews look good, the material extremely relevant. It's all about data analytics using R. I enrolled using my Google account. The only downside appears to be that the course finished on 24 Aug, but all the material seems to be there.


------------------------------
Notes from Thu 31 Aug

Not much time this morning. I'm a bit sick and deliberately slept in and then I did the Vuelta Daily Update.

With the small amount of time available I did a bit of research into MOOCs on predictive data analysis. This one from MIT is looking like the most relevant.

https://www.edx.org/course/analytics-edge-mitx-15-071x-3

I'm now off to check out the hole for our pool at Antill St being excavated!


------------------------------
Notes from Wed 30 Aug @ work

I had a quick go at extending my basic GC prediction analysis by running a random forest (rd) predictive model. The results appear similar to the simple linear regression.


------------------------------
Notes from Tue 29 Aug

I feel like I made some rapid progress for a day or so and then have come to a grinding halt again. The epiphany I had last Thursday regarding the creation of new variables was excellent. It allowed me to create complete input variables for every rider, without NAs, hence allowing regression analysis to be run.

I guess I shouldn't be surprised the resulting models aren't all that fantastic. If there was a direct linear model between past TdF performance and future then the results would be intuitively easier to pick each year. 

I've had a go at using a predictive regression model using four methods so far (with quoted mean deltas): glm(21.33333), ANFIS(39.38889), BstLm(37.58333) & GFS.THRIFT(52.91667).

I guess I should be more positive about the learning experience. This small dataset with fairly simple inputs. I've worked out I need complete datasets and I'm about to work on testing accuracy more thoroughly.

Next steps:

- Further running of current analysis with different models

- Work on method for testing accuracy of results (and understanding it).

- Develop a larger set of input predictor variables.


------------------------------
Notes from Mon 28 Aug

I've had a go at using anothe regression model, on this occasion using the ANSIS package. The results were less accurate it seems, with a larger mean delta between my predicted and the actual finishing GC position in 2016.

This is fun. More to do!


------------------------------
Notes from Sat 26 Aug

Okay - so I've got a basic linear regression predictive analysis up and running. That's a big deal I think! Although the results are embarrasingly bad, this is still a significant milestone and I'm proud to be at this point. 

Using GC position for the past six years, I created a new dataset that has the GC position for each rider in the 2016 Tour de France, followed by my new predictor variables: GC_Complete, GC_Mean, best_GC and worst_GC. 

The good news is the linear regression analysis (using the caret package and "glm" method) was able to run successfully. The bad news is the results are (predictably - no pun intended!) pretty ordinary. The linear, straight-line model has an intercept of approximately 11 with the most influential predictor variables (Best GC and Average GC) both having a positive influence. This means the highest GC position can only be 11!

Next steps:

- Further running of current analysis with different models

- Work on method for testing accuracy of results (and understanding it).

- Develop a larger set of input predictor variables.

- Sort out calculations for the rest of the input categories/variables listed below.


------------------------------
Notes from Thu 25 Aug @ work

Last night I woke up and had some ideas of how to improve my approach for predictive analysis of the basic TDF results data I've collated. I realised I need to build something better suited to a regression analysis, and this means coming up with some categories that (to the greatest extent possible) avoid having NAs.

Here are some new categories I feel could be created:

- Number of completed tours (from the last five years)
- Number of podium finishes
- Number of top ten finishes
- Highest finishing position
- Average finishing position
- Converting yes/no tests into digital 1/0's:
     e.g.
	Finished a specific year
	Positive/Negative trend in finishing position

I'm sure there's more. I've done some preliminary calculations in a new script file:
170825 - Basic TDF GC Regression Analysis.R

Next steps:

- Sort out calculations for the rest of the categories listed above.

- Write code to add these new categories to the dataframe (or a new dataframe)

- Commence initial regression analysis for GC finishing position using new data fields.	

ADDITION FROM HOME. I've had a quick go at some of the above.


------------------------------
Notes from Thu 24 Aug

Little to no progress in the last week as I set up the udpate scripts for the 2017 Vuleta a Espana Velogames comps. 


------------------------------
Notes from Thu 17 Aug

I'm progressing slowly with developing a method for predictive analysis of my basic TDF dataset. The inclusion of NAs in my data (from TDF years in which the rider did not compete or finish) is the big sticking point.

This morning I've reduced my analysis to results from the previous year and a logical vector showing TRUE/FALSE for whether the rider competed the previous year. Unfortunately, if they didn't compete (and therefore have an NA in the finishing position) then there is essentially nothing to predict on and my first model (using glm) is only giving me predictions for riders who raced the previous year. 

Lots more work to do.


------------------------------
Notes from Tue 15 Aug

I'm working on sorting out some basic analysis of my initial dataset. It appears my primary hurdle is overcoming how the large number of NAs are treated. I don't really want to impute a value for the NAs, as I don't think a remotely appropriate value will be determined. I don't want the NAs removed, as the result set is reduced to an almost uselessly small dataset. Either way - the predicted results are heavily influenced.

It looks like I may need to deal with predictive analysis outside the caret package, as I'm finding stuff online that says caret doesn't deal with NAs very well. Anyway, more research and testing required. I've added the dataset and my list of caret analysis packages to the git.

Tutorial on 5 Powerful R Packages used for imputing missing values
https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/

Breaking Down the process of Predictive Modeling
https://www.analyticsvidhya.com/blog/2015/09/perfect-build-predictive-model-10-minutes/


------------------------------
Notes from Thu 10 Aug

Finally - I managed to work out a method for extracting the table on caret packages. The matrix could be converted easily to a dataframe and then it was simple to transpose the dataframe using t() into the correct format. 

I still can't extract the table column names directly from the html which is frustrating. I've saved the table to a local .csv file.

Next steps:

- Not sure. Need to look through my previously listed Next Steps. I think I'll be getting back into analysis.


------------------------------
Notes from Tue 08 Aug

I've been working on the frustrating activity of extracting json data from the following website and attempting to get it into a dataframe in R:

https://topepo.github.io/caret/available-models.html

I've finally been able to isolate the json data and now convert it into a matrix in R. I next need to work out how to convert this into a neat dataframe. The matrix isn't correct. It's got lists of what should be row data.

Next steps:

- Sort out how to convert json derived matrix into a neat dataframe.

- Continue on developing predictive analysis for basic TdF overall GC results data.


------------------------------
Notes from Sat 05 Aug

Okay, off to a rocky start. The first main problem is the data I've got contains plenty of 'NAs' and this means most of the the predictive models I'm using spit the dummy. If I reduce the dataset down to those riders who have been in every TDF for the last six years I'm left with only 9 riders. So I need to investigate predictive modelling methods that allow a bunch of empty data fields. Or at least determine a method that appropriately includes this type of data in the analysis. It might require more pre-processing, such as inserting some sort of data field for NA results. I don't want to get into the business of predicting the empty fields, as I think this is a slippery slope likely to invalidate my analysis pretty quickly.

But it's great to be actually off and attempting analysis. Good fun!

The second drama I've encountered this morning is just a coding one. I thought it would be straight forward to go and extract a table with available predictive methods for the caret package. However, I'm using rvest and I can't isolate the 'table' nodes in the html. The page has some sort of special floating html widget table.

"https://topepo.github.io/caret/available-models.html"

The weird bit is that a 'table' node appears when I'm inspecting the html code on through Mozilla, but I don't think it's actually in the source code. Anyway - it's a little bit frustrating being hampered on what should now (for me) be a straight forward task.

So I need to do some more research on predictive modelling techniques - particularly dealing with incomplete data sets. This is going to be my daily routine, as riders will have patchy competition histories. In fact - this is probably a fundamental part component of determining influencing factors on TDF performance. For example, whether GC contenders ride the Dauphine or not is a factor I want to understand.

Next steps:

- More from the list below. Continuing on regression analysis and other steps.


------------------------------
Notes from Thu 03 Aug

It's gone pretty well this morning. I was able to isolate the correct stage_id for the final stage of each year's TDF (by collecting all of the stage_id's and just taking the last one) and then with this information I built two queries for extracting a dataframe with the final stage General Classification results for the Tour de France between 2011 and 2016. I used two methods, both of which worked. The first looped through each year and then combined the tables in R. The second was a single MySQL query which had all of the stage_id's built into a single query.

I've now taken this data and built my first analysis subset, which has the Rider and finishing position for the 2016 TDF, and then matches their finishing position (in the table) for previous years between 2011 and 2015. It's already interesting just looking at the results and observing the different finishing positions. It's a pretty messy affair. It's also super clear how few riders have competed in every tour from 2011 to 2016. It's going to be interesting doing some preliminary analysis on this data.

Anyway - it's been great to finally work through a practical subsetting exercise and come out with my desired data subset, ready for analysis. It's been excellent to test my database query skills as well as put into action my data wrangling skills (using packages like dplyr), which are of course going to be critical as things get more detailed on the analysis front.

Next steps:

- Aside from the bigger list below, take my initial data subset and perform some intiial regression analysis to see if there is any discernible relationship between a Rider's overall GC finishing position and their GC finishing position in previous years.


------------------------------
Notes from Tue 01 Aug

Okay - I'm underway with some better queries. In the hour I've had this morning I created a subsetting MySQL query that pulled rows from the race_calendar_master_cdf table that matched a date range and the text 'Tour de France'.  I needed to create a corrected date format (cdf) version of the race_calendar_master table first. I discovered (as far as I can initially tell) that the text search is not case sensitive and apparently doens't mind extra spaces. With this search I was able to identify the race_id values for the Tour De France which I'm taking to the next search.

Unfortunately I've just discovered the race_weblinks_master table on the database is empty. I'm now working on combining the individual year tables, but of course when I go to write them back to the database, I encounter UTF-8 isssues even though I've just pulled this data from the database!!


Okay - my notes from the end of the day. I've had some wins and some frustrations. The main win is I worked out a reasonably elegant method for creating race_weblinks_master table. I combined the existing individual year tables through a UNION command in the MySQL query, which thereby avoided the need to download each table to the R console and combine them before writing a master table back. This also avoids the UTF-8 dramas that seem to be introduced during the process.

I've since been working on subsetting queries. I successfully idenfied the race_id values for the Tour de France within a certain year range. This meant setting up appropriate WHERE conditions for the race_details and start_date fields. I've also learnt how to use the wildcard '%' and LIKE command for fuzzy searches within MySQL queries, which helps a lot.

I'm now stuck trying to elegantly isolate the stage_id for the final stage of each of my race_id range. I had a butcher's method I thought would work, but quickly discovered the suffix 's21' doesn't match up to the last stage. In one case (at least) this is because the Tour only had 20 stages! Anyway - more work to do, but it is improving my MySQL query skills, which is critically important for current and future analysis.

Next steps:

- Continue on current Tour de France subsetting task. Currently looking to isolate correct stage_id's.

- More subsetting practice.

- Continue work on analysis plan.

- Convert 'stage_date' in database to 'date' type. More testing on dummy tables required before changing master_results_time & points tables.

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.

- Preparation for initial subsetting of analysis data and simple predictive analysis.




------------------------------
Notes from Mon 31 Jul

Getting prepped for a more specific database search. I'll be looking for Tour De France, Stage 21 Overall Results for years 2011 through 2016. 


------------------------------
Notes from Tue 25 Jul

Only a short session this morning. I'm quite sick and only got up when Liz got up for her run. 

I loaded up the huge (650MB) "super_join.csv" file that has a triple MySQL join between all of the calendar, race weblinks and results tables.  It naturally sucks up a fair bit of memory when loaded. 

This morning I've been practicing some subsetting. From the super_join table I narrowed it down to Tour de France results for Chris Froome (and Christopher Froome) in specific years. I needed to convert the 'date' column to class date.

Next steps:

- More subsetting practice.

- Continue work on analysis plan.

- Convert 'stage_date' in database to 'date' type. More testing on dummy tables required before changing master_results_time & points tables.

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.

- Preparation for initial subsetting of analysis data and simple predictive analysis.


------------------------------
Notes from Sat 22 Jul

Right-o. Not the most focused TDF_Predict session, but productive and very useful none-the-less. I've been watching last night's Tour De France Stage 19 on the Study PC at the same time. Although it was a flat stage with no impact on GC, a large breakaway managed to stay away and Edvald Boasson Hagen did a huge solo effort to escape within the last couple of kilometres to take the stage victory. 

This morning I've been looking to get my 'super join' to work, which is a join between the three key tables:

race_calendar_master
new_race_weblinks_master
master_results_time

I initially ran the query with the master_results_time_cdf (corrected date format) table which returned very quickly a table with just over 19,000 rows. Obiously a lot smaller than the +2.5M rows in the master table. I eventually discovered the master_results_time_cdf table is a subset of the entire master_results_time table, probably because I was using it to experiment with sorting out the format of the date column.

Anyway - a new super join query is running now (it has been for quite a few minutes). It will be interesting to see if it runs okay, the memory is able to handle it and if we get something sensible out at the end.

Next steps:

- Testing on super join query.

- Continue work on analysis plan.

- Convert 'stage_date' in database to 'date' type. More testing on dummy tables required before changing master_results_time & points tables.

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.

- Preparation for initial subsetting of analysis data and simple predictive analysis.


------------------------------
Notes from Fri 21 Jul

I've completed the full join between the new_race_weblinks_master table and the race_calendar_master table. The join worked first go and appears at first glance to be okay! The resulting table () is 16,435 rows, which makes sense I think. I need to check that against the total number of rows in the new_race_weblinks_master as I think they should match.

Next steps:

- Running the big and nasty full join between three tables!

------------------------------
Notes from Thu 20 Jul

I've just discovered the master_race_weblinks doesn't have the completely updated data. The master file only has four columns (stage_url, stage_id, race_id, race_details) where the updated individual year race weblinks tables have eight.

"Stage"        "date"         "location"     "distance"     "stage_url"    "stage_id"     "race_id"      "race_details"

So I've been working on solving this by writing a new race_weblinks_master table to the database, using the following script:

170720 - Race Weblinks Master - create new master table.R

I've run into a weird problem. I had no problems pulling all the individual tables from the database and putting them into a new master dataframe, however I can't successfully write this back to the database. It's throwing it's old invalid UTF-8 character fit, and it appears to be all on the first "Stage" column which has a range of descriptive titles in there. I think there are a range of unacceptable characters in there ("\", etc) but this is weird given I think they were in the database in the first place! More work to do.

Next steps per below.


------------------------------
Notes from Tue 18 Jul (short night-time session)

I've had a quick go whilst watching MasterChef with Liz. In the Initial Database Analysis file I've inserted a quick MySQL join script that seeks to perform a complete join between the race_calendar_master and race_weblinks_master tables. I don't have the Study PC turned on so I haven't run it on the database.

Next steps per below.


------------------------------
Notes from Tue 18 Jul

I've been doing a number of things today, all with respect to database queries and organising data for analysis. 

170705 - Initial Database Analysis.R

The first was checking the 'race_calendar_master' table against all the individual year race_calendar files. I was able to confirm the name number of entries/rows for each year, the results stored in the following file:

home/a_friend/data_analysis/projects/TDF_Predict/working_data/master_calendar_count.csv

So it looks like I can use the race_calendar_master table for my analysis.

The other activity I've been working on today is creating a JOIN between the race_calendar_master, race_weblinks_master and master_results_time tables. This is required as I don't think the name of the race (race_details) is stored in the race_weblinks_master table (need to check) in addition to the fact I'm sure I'll need other information in the race_calendar_master table at some point (such as race discipline and location). 

I initially thought I need to run two separate JOIN queries or do something clever within R, but it looks like multiple joins can be executed within MySQL. 

This forum post has details, although it's not clear whether it works for them!
https://www.reddit.com/r/mysql/comments/5zj92d/joining_three_tables_together_while_using_a_where/

I've got a query running now and of course it's been taking a while...

The join query finished.... and WORKED!!! Hooray!!

Next steps:

- Run a bigger super join query.

- Continue work on analysis plan.

- Convert 'stage_date' in database to 'date' type. More testing on dummy tables required before changing master_results_time & points tables.

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.


------------------------------
Notes from Sun 16 Jul

I've had some success today attempting to sort out dates within the MySQL database. I've been working with the following file:

170705 - Initial Database Analysis.R

I've been doing work on a (reduced rows) copy of the master_results_time table, called master_results_time_cdf.

The good news appears to be realtively straight forward to convert the format/type of the stage_date column, using the following MySQL query.

dbSendQuery(conn_local, "ALTER TABLE master_results_time_cdf MODIFY stage_date date;")

Once I've done this it's been possible to perform standard MySQL queries filtering for dates and date ranges.

Next steps:

- Check whether my big JOIN query is still sitting on the database server.

- Draw up analysis plan.

- Convert 'stage_date' in database to 'date' type. More testing on dummy tables required before changing master_results_time & points tables.

- Check Race Calendars. Combine into master calendar

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.

- Start searching, filtering and summarising data. Get back up to speed on MySQL skills.


------------------------------
Notes from Thu 13 Jul @ work

I've started an analysis plan and added it to my git repository.


------------------------------
Notes from Thu 13 Jul

Time to start working with my data. I've decided it would help having an initial analysis to conduct, and to use this as a real test case for collating the required data. I haven't written out the hypothesis until now:

Hypothesis: The final rider finishing position in prior editions of the Tour De France is correlated to the final finishing position of the rider in the upcoming Tour De France race.

The first thing I've identified is the data I will need is spread across several tables (but I knew this would be the case when I constructed the ProCycling database). The master_results_time table doesn't have the race name (race_details) which is contained in the master_race_weblinks table and originates from the race_calendar table. So I'm immediately going to need to start working with MySQL join commands. I've got one running at the moment, which is taking a long time - most obviously because I didn't limit the query and it's looking at all +1M rows of the master_results_time table....  

The result was the query looking like it was going to run pretty much indefinitely, so I had to force R to close in the end. Which led me to attempting to find out if the query was still running on my MySQL server. Apparently I'm meant to use the following MySQL commands to show open queries and kill off the ones I want to close:

show processlist
KILL QUERY **Id** where Id is connection id from show processlist

Anyway - more work to do. 

Next steps:

- Check whether my big JOIN query is still sitting on the database server.

- Draw up analysis plan.

- Convert 'stage_date' in database to 'date' type. Work on some dummy tables first.

- Check Race Calendars. Combine into master calendar

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.

- Start searching, filtering and summarising data. Get back up to speed on MySQL skills.


------------------------------
Notes from Tue 11 Jul

I've got the entire dataset in the database (as of Saturday)!! I'm now commencing initial preparation for analysis, which at this stage is only getting to grips with pulling and subsetting data from my database.

The first problem is I will almost always be using a date subset, and I've immediately discovered my dates are stored as character type and aren't therefore available for MySQL date functions.

I'll have to think about my approach for this. After plenty of messing about, I discovered a straight forward MySQL conversion that changes the 'stage_date' value into the correct format:

convert(stage_date,date)

I can use this on queries (at least I've had it work for filtering on a single specific date). 

I need to decide whether I convert the entire date range in the database permanently to date type, or leave it as a character type and do the conversion everytime. I think the permanent change is the best answer, but I'll need to do some testing before making adjustments to the main dataset.

Next steps:

- Draw up analysis plan.

- Convert 'stage_date' in database to 'date' type. Work on some dummy tables first.

- Check Race Calendars. Combine into master calendar

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.

- Start searching, filtering and summarising data. Get back up to speed on MySQL skills.

- Time to get into analysis. I probably need to develop a plan for this.


------------------------------
Notes from Sat 08 Jul

I'm about to run the last year (2017) of the race table cycling news scrape to update the master tables and therefore complete the first important stage of my TDF Prediction project. Yay!! I'm currently just doing an update to the race weblinks data so I've got up to the lastest race for 2017. I started the race table webscrape before I realised I needed to do this and got through 6 rows before stopping it. I'll have to go and delete what I presume will be duplicates from the master tables later on.

Now it's time to start the real business - of analysis the data and working on methods for running predictive algorithms. It's evident from my first attempts and summarising and testing the data I've got my work cut out initially just searching, filtering and organsing the data I want. I'll need to get back up to speed on my MySQL skills so I can join tables. I'm sure I'll also be tested regarding the accuracy and consistency of this data. I think developing some sort of decent fuzzy search will be necessary in order to correctly obtain data - and example would be for riders having variations on their names, either through spelling mistakes in the raw data or (more likely) the alternative variations on long European names....

Next steps:

- Check Race Calendars. Combine into master calendar

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.

- Start searching, filtering and summarising data. Get back up to speed on MySQL skills.

- Time to get into analysis. I probably need to develop a plan for this.


------------------------------
Notes from Thu 06 Jul @work

I'm testing out working with a git branch. I've created and checkout out a branch called 'test-branch-01'. I'm now making file changes, in this case just to this Daily Notes file. 


------------------------------
Notes from Tue 27 Jun @work

I'm currently sitting in the CASG PBC course.


------------------------------
Notes from Fri 30 Jun

I've just been running the race results script for 2015. It crashes on occasion with the database connection error. This is annoying and I might next try running it from the laptop to see what happens.

I also had a crash on row 1076 of the 2015 race weblinks table, which corresponds to the following results:

http://www.cyclingnews.com/races/bpost-bank-trofee-koppenbergcross-2015/elite-women/results/

This results table (there's only one) has a mix of time and laps in the result column!! I've just skipped it at the moment.....

I've finished running all the results scrape for 2015.



------------------------------
Notes from Thu 29 Jun

Updates to race_results_tables_V2 function. Added text_clean to Country and moved database open and close script to where it is used.

So I worked out the issue with the non-UTF character problem (as usual, not writing to the database). I needed to run the text_clean function on the 'Country' column as well. This normally isn't required, as the 'Country' is normally a CAPS three-letter country code, but in the case of the Old Pueblo 24hr mountain bike race, the Country column contained the team names which were a range of weird and wonderful titles. This race also had the anomaly of 'laps' being used for the result column. I've decided to leave this alone (and not create an entirely new master table in the database for 'lap' result info) as I'm unlikely to ever need this data. It gets categorised as 'pts'. There are four-man teams in some of the tables, which means the 'Rider' column has four riders. Again, I'm not worried as I don't think I'll ever need data from races like these..... (famous last words!)

I'm still writing race results data to the 'test_test' tables in the database. Once I'm happy the function is scraping effectively, I'll do some testing on the results and hopefully the use of the data. Perhaps some prelimary summarising and plotting. If that works out then I'll sort out doing a clean scrape of all the race results data....

The Tour De France starts in two days. My target was to have analysis up and running and possibly even having a go at some predictions at this point. I'm a long way behind that, but it's still satisfying to be on the cusp of having the complete database of race results stored and ready for analysis. Phew!

It's hard to know what to do with the problems I'm having with the connection to the database. I'm finding it difficult to isolate what's causing the error. I feel like my database connection code is nice and clean. Anyway, I'm trying a fix where I've moved the database connection and close script to directly where it's used in the race results function. Opening and closing the connection exactly when needed. 

Next steps:

- Further testing of race results scraping function. Look out for database connection error. Once a large amount of data is scraped (perhaps a few years of results), test out the stored data for correctness and then use in analysis.

- Check Race Calendars. Combine into master calendar

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.






------------------------------
Notes from Tue 27 Jun

Firstly - I've changed how race time is handled by the race results table function. I've made the following adjustments:
a. I've decided to retain the original time entry from the website, in the original 'Result' column
b. I'm converting the time using lubridate::seconds (new column 'result_seconds') and then leaving the value as an integer (seconds)
c. Creating a new 'duration' column with the correct cumulative number of seconds for each rider.

I've addressed a variety of issues with the race results tables function and I'm sure I'll run into a few more use case issues with the new functionality I've added. 

I'm having problems with connections to the database (too many connections). I've fiddled with the close all connections script, but in the end I restarted the database.

I'd addressed an issue by extending the text_clean function to the 'Team' column and the captions and I inserted a gsub to remove the parentheses on the 'Country' column (occurs when there is no team).

Inserted IF script to deal with duration conversion on tables with only one row....

Next steps:

- Attempt to work out why I'm getting the following error:
 Error in .local(conn, statement, ...) : 
  could not run statement: Lost connection to MySQL server during query
It's weird because it doesn't look like there are too many connections and if I start running the script again (just prior to the error point) it continues on its merry way.

- Looks like there are more invalide UTF-8 issues with the race results tables function. Latest is at Row 175 of Race_Weblinks 2013.
http://www.cyclingnews.com/races/24-hours-of-old-pueblo-2013/results
This race is a MTB endurance race and has tables with 'laps' in them!!! Arrrghh!

- Check Race Calendars. Combine into master calendar

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.


------------------------------
Notes from Sat 24 Jun

I've spent the morning running the race weblinks function through the 2013 calendar entries. I've run into some problems, but it would appear the script wasn't dealing with them thanks to the updates I'd lost through merging to git.

Anyway - I think I've got it running again. I needed to insert two items back:

1. A logical test at the first IF statement to check the race url is longer than 7 characters. The error I've encountered is some race links are pointing to the head cyclingnews.com/races page, which is both live and contains an overload of stage data.
2. Some weird race tables are not binding with the race master. I decided to avoid having to come up with some complex solution for very rare (and uneeded) cases by inserting a tryCatch() around the rbind.

I've now got the race weblinks function running again for 2013.

Simultaneously I've been working on how to deal with durations in the master_time table. I wrote a small function at work designed to convert durations (stored as strings in the database) back into a time format. That seems to work, but I'm considering what the best method for storing will be long term. It might be preferable to just convert all durations to seconds (at the moment of source) and then convert to a numeric or interger, and then store them as a number in the database. 

Next steps:

- Integrate my new duration conversion into script accessing the database.

- Possible consider storing race times as lubridate::seconds instead of lubridate::duration.

- Check Race Calendars. Combine into master calendar

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.


------------------------------
Notes from Thu 22 Jun

I'm apparently still muddling through my use of git when it comes to multiple computers. This Daily Notes file is the best example as I managed to lose a couple of days notes when dealing with out of phase updates between the HP Laptop and Study PC. I didn't think I was working on different files or really even updating stuff at different times. Anyway - more learning there which I'll defer until after I've got the full dataset scraped for the first time. 

I believe I've sorted out converting durations stored in the database back to the correct class/format. Sadly, I haven't discovered an elegant function that does this easily. It's a bit frustrating that a lubridate function doesn't do this. There's some irony in the fact lubridate recongnises time/date formats in strings in so many formats.... other than it's own duration!!

My new script for dealing with duration is in the following working file:

170622 - Working with lubridate duration.R

Next steps:

- Integrate my new duration conversion into script accessing the database.

- Possibly consider storing race times as lubridate::seconds instead of lubridate::duration.

- Scrape 2013 race weblinks data

- Continue working on how to record error and warning outputs (tryCatch) (I think I've got this going)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.

- More to learn on merging git branches.




------------------------------
Notes from Sat 17 Jun

I've updated the race weblinks script to include the tryCatch() function. This is meant to create a method of capturing errors in an error log file (weblinks_error_list.csv) and allow the script to continue scraping further weblinks.

I've just watched the script run into a warning at row 21 of 1274 for 2013 and it's printed my warning script to the console and continued to the next row. I've just checked the error log and it's empty! Time to check whether I create a blank error log everytime a new row is started.....

Upon running the new function for 2013, it's caused R to abort when running Row 43. I've just it a second time and the issue happens at exactly the same point. This is frustrating as it's very difficult to determine what is causing the abort.

I've looked at the diagnostics reports (which have nothing useful) and tried emptying the Global Environment. I really don't think it's a memory issue as the system memory is at less than half capacity during the script execution.

I'm now doing an update to both R and R Studio to see if this helps.

Next steps:

- More to learn on merging git branches.

- Work out how to convert back Duration values stored in the database results tables (currently a character).

- Continue working on how to record error and warning outputs (tryCatch) (I think I've got this going)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.






------------------------------
Notes from Thu 15 Jun @ work

I've had a crack at improving my understanding and use of the tryCatch() function and made some progress. The solution to capturing the error/warning apparently seems to be putting a robust file access solution into the tryCatch() function, in this case accessing a .csv file and then writing new error/warning message to this file.

Next steps:

- Sort out why it's only recording the first warning message (but still recording the year and error counter numbers just fine. It's recording <NA> of the rest. 

- Looking at extending this to my TDF webscraping functions.




------------------------------
Notes from Thu 15 Jun

A fairly unproductice morning. A script I was running last night for the race results tables for 2010 was interrupted. Then the computer restarted. I'm not sure where that script got up to (I guess I can check in the database) and the computer took ages to reboot completely. I started using the laptop and then this lead to challenges merging my GitHub files. I spent most of the morning learning more about merging and recovering files on the git. I've got more to learn!

It is positive to have so many results now populating my master_results_time and master_results_points tables in the database. I did a quick check and the master_results_time table has 827,531 rows/entries!! (And I'm only up to 2010). I did a quick dplyr filter on Richie Porte (which worked) but then it became quickly clear I need to work out how to convert the time values back into the correct class as they are stored as a character, in the unique lubridate format. 

Next steps:

- More to learn on merging git branches.

- Work out how to convert back Duration values stored in the database results tables (currently a character).

- Continue working on how to record error and warning outputs (tryCatch)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Wed 14 Jun

I'm running into a number of issues trying to close out my race weblinks scrape and completing writing them to the database. The most troubling one is RStudio aborting in the middle of the race weblinks script running. 

I solved a more minor issue today. The script stopped with an error on a webpage containing a funny mix. It had a table with past winners of the race (not a table with stages and race results weblinks) and then the stages in the usual webpage format. So my script isn't built to deal with tables on the page for another purpose.

Anyway - I think these will be in the vast minority. This race was the USA cyclocross championships in 2016. I've decided not to try and solve this specific issue and instead implemented the use of the 'try()' function. I wrapped two 'try()' functions around "Fork One" and around "Fork Two" do deal with errors being thrown out. Ideally I want to use 'tryCatch()' instead but I've not yet worked out how to write the location of errors to table. Essentially my own version of an error log. This would allow the script to progress without stopping for errors, but then giving me details of the error location at the end so I can go and fault find. I'm not so worried about the race weblinks function, as I think I've got all of the needed races, but I want to make sure I've got a 100% solution for the race/stage result tables scraping function.

The race weblinks function is now running for 2013. I have 2011, 2013 and 2017 to complete. I'm working from home this morning and will hopefully be able to run these while I'm working.

Next steps:

- Continue working on how to record error and warning outputs (tryCatch)

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Tue 13 Jun @ work

I've been experimenting with the TryCatch() function, which has been going okay. I've not yet worked out how to record the errors (to a dataframe). Apparently any executed work within the error function stays inside.

The good news is that with either 'try' or 'tryCatch' I should be able to run long webscraping scripts and have them continue, even if they run into an error. 


Next steps:

- Continue working on how to record error and warning outputs



------------------------------
Notes from Tue 13 Jun

I've just re-run race weblinks for 2011. It needs checking.


------------------------------
Notes from Mon 12 Jun

I ran into problems on Sunday with R crashing (aborting). I haven't had time yet to work out what's going on. It crashed in 2011 and I think in 2013 and 2016. I've written a short bit of script to return the status of race weblink tables from the database. Non-complete tables are obvious because they have only 5 columns (instead of the new 8 column format) and don't  have a full completment of rows.

I think the 2016 run of the race weblink function may have aborted on row 412.

Years to be run for the race weblinks function are: 2011, 2013, 2016 & 2017



------------------------------
Notes from Sun 11 Jun

The latest error kicked up was thanks to one (of ten) tables on a results page missing the column name '#' on the first column. I've decided to do away with the smarts and force the columns names to "#", "Rider Name (Country) Team", "Result". I figure even in the rare occasions this is different, ultimately I want uniformity so the tables will bind correctly and then upload to the database. 

These might be famous last words - but I think I'm close to being ready to scrape the full set of race weblinks and then the full set of race results tables shortly there after. 

### I was running from 2006 to 2012 and hit my first error in the middle of 2009, which is pretty good really. The issue was an 'Article 404' webpage. I've inserted 'RCurl::url.exists(race_url)' into the 'stages' check, which will hopefully sort it out.




------------------------------
Notes from Sat 10 Jun

I've been working on the race results tables function. I encountered an error last time at row 24 of the 2014 weblinks table. The use case error could be traced to a non-standard column for the Rider Name (Country) Team. In this case it was the Australian national championships and the column was titled 'Rider Name (State)' with no 'Team'.  I've decided to force the column title to match all the others, firstly to ensure uniformity across all the race results tables and to make sure these tables are combined and loaded up to the database without issue. 

Next steps:

- It would be really good to sort out the trycatch() function so I can run long web scrpaes and have the errors returned for sorting out afterwards.

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Work out a secure connection process for accessing the database from offsite.




------------------------------
Notes from Thu 08 Jun - Evening

I've just tried running the get weblink function for calendar year 2010. It's immediately run into some new use-case errors:

- Non-complete race weblink data, resulting in less race weblinks than stages.
    example at row 14: http://www.cyclingnews.com/races/czech-cyclo-cross-championships-cn/stages/
- Error 404 non-existing page
    example at row 39: http://www.cyclingnews.com/races/croatian-cyclo-cross-championships-cn/stages

I came up with butcher's solution for the first one - repeating the list of results weblinks to fill the number of stages. I figure this will only occur on rare occasions for smaller races. The end result will be by result scraping function will still extract all of the available race result information. The bad news is that the result weblink may not always perfectly line up with the correct stage information....

The second issue is what I need to work on next, and coincidentally will likely require the 'trycatch()' function I was planning on implementing anyway. There appears to be a specific guide on using trycatch() with Error 404 problems at the following link:

https://protocolvital.info/2016/04/21/404-error-handling-with-rs-download-file/

Next steps:

- Check Race Calendars. Combine into master calendar

- Continue evolution and check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Improve scraping code to identify errors and report them. Function 'trycatch()' might be an option.

- Work out a secure connection process for accessing the database from offsite.



------------------------------
Notes from Thu 08 Jun

So it looks like I've got mostly operational functions to go and get all of the data I require. I started running the raec weblink function for Year 2012 and ran into an issue where the (number of) stage weblinks does not match the number of stages in the Stage table. I found the table had a duplicate stage and I inserted a line of code to fix this issue. I'm now running Year 2012 race weblink again.

I've realised it is important to learn how to write webscraping script that collects and returns data on errors and (if possible) continues running. Essentially, if I'm running through a loop and it fails, so collect identifying information for the failing row and then to continue to the next one.

Need to check 2012 race weblinks completed successfully.


Next steps:

- Check Race Calendars. Combine into master calendar

- Check existing Race Weblink tables. Combine into master. Scrape the rest (which is most)

- Go and get race results tables for all races. Already setup to go into master table. It should be +1M rows when complete!

- Improve scraping code to identify errors and report them. Function 'trycatch()' might be an option.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Wed 07 Jun - Sunrise session

I've firstly dealt with the error generated at row 746. 

 Error in matrix(NA_character_, nrow = n, ncol = p) : 
  invalid 'ncol' value (too large or NA)

I discovered the last table in the following page is empty
http://www.cyclingnews.com/races/prudential-ride-london-classic-2015/ridelondon-classic/results/

Despite being empty, the annoying table has the sub-node 'tbody', which was my previous discriminator for eliminating empty tables. I've now extended the xpath identifier to contain '/tr' as well:
xpath="//table[.//tbody/tr]"

I'm a bit worried this might exclude some tables I want to keep, if they are missing the '/tr', but I actually doubt there are any of these.

I'm now running the rest of the race calendar. I will, however, have to carry out thorough accuracy testing. I think it actually works! Lots of testing to do yet.....

Next steps:

- Work out a secure connection process for accessing the database from offsite.

- Look at combining the race weblink table into a single master table.

- Work on combining the calendar tables into a master table. 

- Work out a secure connection process for accessing the database from offsite.




------------------------------
Notes from Tue 06 Jun - End of day

The race result table script has been running relatively well. The latest error is at row 746.


------------------------------
Notes from Tue 06 Jun - Midday

I inserted some code dealing with completely empty 'Result' columns, including no 'Result' column name. Now we're getting somewhere. I started the complete calendar test again and reached row number 85, which is the following event result:

http://www.cyclingnews.com/races/etoile-de-besseges-2015/stage-2/results/

The issue is almost certainly that the Sprint 1 table has no rows (after the columns headings), because the sprint was nulified. I'm sure we'll come across this again. So I need to sort this use-case out too.


------------------------------
Notes from Tue 06 Jun - Midday

Going okay. I sorted the early error, but have now run into the special case of race results tables with no times or points. In this case the track world championship Keirin event!
http://www.cyclingnews.com/races/uci-track-world-cup-iii-2014-1/day-1/results/

I think the lubridate duration function doesn't like being passed NULL values. 


------------------------------
Notes from Tue 06 Jun - Sunrise session


I think I've a reasonably elegant solution to the task of only extracting tables with content. By modifying the xpath selector I'm able to only extract table nodes with the correct child element, in this case a 'tbody' node. 

html_nodes(xpath="//table[.//body]")

So I started running tests and got plenty of problems, but I think I've managed to sort most of them. I'm about to run a bigger test on a whole calendar year, while we go and have breakfast. I anticipate problems!!

I just started it and got my first error stopping everything - at row 6!


------------------------------
Notes from Mon 05 Jun

I think I've sorted the issue of dodgy webpages. I ran my test again and it progressed past the nasty extra 2012 Paris Roubaix race. 

Next problem:  Some result pages apparently have empty table nodes. The one that's causing me grief at present is:
http://www.cyclingnews.com/races/gp-de-denain-porte-du-hainaut-2015/results

It returns to tables (even though there is a single results table on the page). The first table node is empty, and is causing an immediate error in the rvest 'html_table' function towards the top of the function_race_results_table_V2 function.

Next steps:

- Sort out a method for eliminating the empty table.

- Continue on the Next Steps list below



------------------------------
Notes from Sat 03 Jun

I've hopefully solved some interesting problems today.

The main one was I found and issue writing to the database and traced it to results pages not containing and 'points' results. The one I came across was for the Ronde van Drenthe race in 2015 which only has a single results table, for finishing time. This means the 'points_tables' is empty (NULL) and the dbWriteTable function spits the dummy when presented with no data. This was easy enough to solve with an appropriate 'IF' statement. Good to know, as there will be plenty of these results across the calendar.

The earlier problem metastasised when I was dealing with non-breaking spaces in the 'Result' and 'result_type' columns. I (had previously) inserted a clever

I've run a longer test and I think I've come into the issue of a dodgy URL link:
http://www.cyclingnews.com/races/paris-roubaix-2012/results

The above link has a number of issues. Firstly (and most importantly), it's got the full URL and not just the bit after 'cyclingnews.com'. The second issue is it's for the 2012 Paris-Roubaix and this URL is in the middle of the 2015 Race Weblink table.

Anyway, I think I need to put in a test for URL EXISTS going forward. I can't yet find one that weeds out clearly non-functioning websites....

Next steps:

- Identify and implement a robust url checking decision process

- A full run of the race weblinks functions for all calendar years. (This will take some time!)

- Work out a secure connection process for accessing the database from offsite.

- Look at combining the race weblink table into a single master table.

- Work on combining the calendar tables into a master table. 

- Work out a secure connection process for accessing the database from offsite.



------------------------------
Notes from Thu 01 Jun - Day Session (WPD)

I'm about to take a break to go and pick up my wheel from Tom Wallace Cycles. I'm having problems with the race results tables function, I think due to issues clearing the database connections...


I've since identified that the issue is the 'Result' column is being created as a list. I had a solution that turned it into an integer, but then this wrecked the 'time' based Result values.

I'll need an IF statement of some type.





------------------------------
Notes from Thu 01 Jun - Sunrise Session

I'm running a test of data from the new race weblinks data being fed into the race table extract. I'm getting a number of issues:

- Problems writing to the database with the updated race table data
- Issues with my script splitting 'points' tables from 'time' based tables.

Next steps:

- First I'm working on sorting out the 'points' vs 'time' split



------------------------------
Notes from Tue 30 May

I've made further refinements to the race weblinks function to correct errors and improve the outcome. 

The main initial error not having 'dplyr' called as a required package at the start of the function. The first time the function was loaded cleanly and needed to 'filter' some rows the code threw out an error that was initially hard to identify. Having 'rvest' loaded meant the piping was working, but the dplyr specific functions were not. I guess when I was writing and testing development of this function I probably had dplyr loaded into the environment. It's definitely been useful working from several computers and getting the code to robustly execute.

Improvements included pulling through the race location from the calendar file for stages where no location data is scraped from the race or stage webpage. I also inserted the 'text_clean' function to uniformly sort out location data.

There was already the usual include script for a delay between webscraping. 

I now need to look at combining the race/stage weblink data for individual years into a master table. This should be able to occur immediately as I think each row entry is already unique and has a PRIMARY KEY column (stage-id). With the dates included for every row, it will be open to immediate filtering and analysis.

I've just run a test of the function using the full 2015 race calendar. I'll save this Daily Notes file before I see the end result.

Next steps:

- A full run of the race weblinks functions for all calendar years. (This will take some time!)

- Work out a secure connection process for accessing the database from offsite.

- Look at combining the race weblink table into a single master table.

- Work on combining the calendar tables into a master table. 


------------------------------
Notes from Mon 29 May @ work

Further testing and refinement of 02_function_obtain_event_race_results_weblinks.R.

Some of the issues I've dealt with include:
* Some race pages not having any results at all (!!) - e.g. Milan San Remo 2015
* Some race pages with result links not having a date (at least in the right format)

Next steps:

- Insert time delay script for webscraping

- Further testing and fault finding on 02_function_obtain_event_race_results_weblinks.R

- Work out a secure connection process for accessing the database from offsite.



------------------------------
Notes from Sat 27 May

I believe I've got the race weblink extraction function (#02) reasonably functional. I need to do some more testing.




------------------------------
Notes from Fri 26 May @ work

I've made some improvements to the race weblinks function that essentially deal with the scenario where race_links = 0, i.e. there are no race weblinks. Some 'if' statements did the trick.

I've just tested the function again and it's improved, but it's encountered an error with the 2015 TDF. It can't see the column "results" in the extracted table. I need to look at whether it's picking up an incorrect table as the one loaded in has overall race results....

Next steps:

- Further testing and fault finding on 02_function_obtain_event_race_results_weblinks.R

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Thu 25 May @ work

I made good progress on developing the race weblinks function and think I have it close.
Initial testing has been positive. I've run into an early issue with testing on the 2015 TDF.
It appears the race link for the TDF goes to some form of "countdown page":
# http://www.cyclingnews.com//races/2015-tour-de-france-countdown-2015
This appears to be more of an issues for the race calendar function than the race weblinks function.
Anyway - I'll keep testing and see where I get to.

Next steps:

- Continue testing on updated race weblinks function: 02_function_obtain_event_race_results_weblinks.R

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Thu 25 May

I'm making progress on the 02_function_obtain_event_race_results_weblinks.R file. I've inserted IF/ELSE statements and the code to deal with date formats in both table and non-table arrangements. At the moment I'm sorting out the non-table format.

Next steps:

- Finish insertion of new code into race result weblink function dealing with alternate storage of date data. Test and debug.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Wed 24 May @ work

I made some updates to the following test script, in support of sorting out the race weblinks function

170523 - Test script for extracting race weblinks.R

I managed to clean up the table-based data extraction:
- Deleting Rest Day rows
- combining table with weblinks


------------------------------
Notes from Tue 23 May @ work

Just when I thought I'd licked this one I discover and issue and open a can of worms. Of course data for multi-race/stage races are stored in different formats. The main obvious difference is that stage races (Grand Tours, etc) are stored in a table, whilst multi-event races (such as National Championships) are stored in a divided page layout.

Next steps:

- Continue work I'm progressing on extracting the data I need from table-based pages.

- Create some sort of test (if statement possibly) to distinguish between the two types.

- Carry out testing. I possibly need to send a race calendar file to work.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Tue 23 May

I managed to fix the main issues with 02_function_obtain_event_race_results_weblinks.R regarding the date extraction. I inserted a suitable bit for single dates (I needed to adjust the html reference and adjust the format of the lubridate conversion to handle dates with a time as well as a date).

Next steps:

- Do a bigger test of the improved 02_function_obtain_event_race_results_weblinks.R function.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Sat 20 May

I carried on testing and improving the race result weblink function. 

Still working on the following function:
02_function_obtain_event_race_results_weblinks.R

I'm running a limited test (10 rows from 2011) and the function is not returning a correct table. It's not returning the target year data and it's missing the new stage_date column

Next steps:

- Assess changes to race weblink extract function and whether dates (and raceweblinks) are being pulled in accurately). 

- Fix missing single date races and ensure dates are being converted to correct class.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Fri 19 May @ work


I did some mild messing around with the race weblinks script, but spent most of the time trying to sort out the issues I had with RStudio freezing on opening.

The good news is that the git process appears to be working well.


------------------------------
Notes from Fri 19 May

I'm getting the race weblinks function updated to pull in the stage dates as well. There are a couple of challenges I've identified. 

The first is the stage dates are apparently displayed only once on the web page, but the race weblinks are usually duplicated. This means when I combine the two sets of lists into a dataframe, they are difference lengths. At the moment the dataframe stops building after it runs out of dates and (as far as I can tell from a short test) I end up with the correct table. However this is a bit of a butcher's method and I don't like it.

The second problem (I'm not certain it's a problem yet) is what happens when there are no race weblinks. I think the stages just get ignored, but I'm a bit worried the previous races' dataframe will be pulled in. 

The first test (for 2011) has just finished and it largely looks successful. The most obvious issue is that single date races (i.e. not stage races) don't have a date. This is probably because a date isn't listed on the race webpage. Dates are also in a straight numeric class. 

Next steps:

- Assess changes to race weblink extract function and whether dates (and raceweblinks) are being pulled in accurately). 

- Fix missing single date races and ensure dates are being converted to correct class.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Thu 18 May - at work

First clone of TDF_Predict git repository from work computer.

Next steps:

- Ensure git is working correctly.


------------------------------
Notes from Thu 18 May

I was away in Canberra for the first part of this week. I did not have any success remotely connecting to my database on the home Study PC.

I've started working on the "02_function_obtain_event_race_results_weblinks.R" function to pull in stage date information so it can be used later in the race result table.


Next steps:

- Continue working on modification of race weblink function to pull in stage date information.

- Work out a secure connection process for accessing the database from offsite.


------------------------------
Notes from Sat 13 May

This morning I've worked on allowing remote connection to my MySQL database. The primary actions performed were creating a function that sets the location of the database password file on both the Windows PC and the Linux laptop, and to modify the race_results_tables_V2 file to point to the results of this new function:

Next steps:

- Test remote connection from outside of home WiFI to see if I'm going through the firewall okay.

- Start working on testing webscrape of race results tables.


------------------------------
Notes from Thu 11 May

Today I've been continuing to work on the race result tables. I'm attempting to work out how to build the time-based 'Result' column into the correct race duration for each rider. I successully eliminated the 'non-breaking space' characters from the Result column, but I'm now trying to work out how to mutate a new Duration column with the correct cumulative time for each rider.

I've been able to correctly assign new columns for race_type and race_classification.

Next steps:

- Continue working out how to build a 'Duration' column

- Correct non-finishing entries (e.g. DNF, DNS, DNQ, etc).

- Combine all 'time' tables and 'points' tables into two master tables.


------------------------------
Notes from Wed 10 May

Wet weather in Brisbane this morning, so no 4501 Rouleurs River Loop. Hopefully it will be dry enough for me to ride into work later.

After doing the Giro daily update, today I worked on further sorting out the race result tables.

The cool bit I worked out today is how to split a combined column (in this case "Rider (Country) Team" into separate columns using the 'separate' function from Hadley's 'tidyr' package. It took a bit to understand the syntax and identify the correct regex expression for matching special characters (in this case the parentheses). 

Next steps:

- Combine the script I developed today to separate columns with the other new script I've developed recently to use rvest to download race result tables, correctly assign headers and assign columns for pts/time and race_id.


------------------------------
Notes from Tue 9 May

I've been distracted by getting daily updates for the Giro Velogames competitions up and running, which is now going well. Time to concentrate on the main game!

I believe I've got a format and solution for extracting and storing race result information. The two big decisions are:

1. I will store all of the TIME based results in one table, and all of the POINTS based results in another table.
2. Each table entry will hace its 'result class' (full, general_classicification, sprint, points_classification, etc) stored in each row (in-line) as a field.  There will also be a field for 'result_type' which will simply state whether the numerical result is a 'time' result or a 'points' reult.


So this means all of the time based results (full stage result, GC, teams, young rider, etc) will be thrown into one big table. The results will be separable via the race_id (also a field) and the result_class.  




------------------------------
Notes from Thu 29 Apr

I've started looking at how to sort out the storage of race result tables. I also today identified it is neater/easier to use the 'dbGetQuery' function instead of using the two-step dbSendQuery and dbFetch. Apparently the above is recommended by Hadley.


Next steps:

- Assess how race result information will be called. If I'm going to combine all the race result tables into a Master table, additional columns will be required to ensure results are unique. The most obvious addition is the unique 'stage_id'.

- Cleaning up the race result data. For example
	* Filling in empty time entries (making sure I don't give a time to DNF, etc
	* Separating data squashed into a single column (Rider Name, Country, Team, etc)






------------------------------
Notes from Thu 29 Apr

I've started sorting out the race results function and working out how this is all going to get stored on the database. First step was trying to execute the race_results_tables function, which has a variety of errors.

I found that my script in 'Test script for running race results' was referencing the incorrect columns from the 'race_weblinks_Master' dataframe.

Anyhow, I've got that sorted, as well as cleaning up the race_results_tables function a bit. I'm still a bit stuck on how to combine the variety of tables into Master results tables.

Next steps:

- Look at how to combine race results data into master tables

- Think about how I'll retrieve race data. Dates may be a challenge if the date isn't stored in the race results tables.


------------------------------
Notes from Thu 27 Apr - PM

I've re-run the race_calendar function successfully and now have a race_calendar_Master table in the database. It appears to have clean race details and locations as well as dates in a useable format. I've done the same for the race_weblinks tables, which I believe has run successfully. I now have a race_weblinks_Master table in the database as well!

Next steps:

- Move on to race results tables. This is a huge challenge and I'm not exactly where to start.
  I need to work out how I'm going to store and retrieve information from these tables for analysis.


------------------------------
Notes from Thu 27 Apr

I'm having more problems than I anticipated sorting out the race_calendar_Master table. I've discovered a couple of challenges:

1. I haven't done a consistent update to the race_calendar tables and as a result there is inconsistent column naming. At this stage I know the earlier ones are named 'Start.Date' and the more recent tables have 'start_date'. I think I need to re-run the race_calendar initial extract function again.

2. The above might solve the following issue. I've converted some of the start_date and end_date values as a test, and then attempted to lubridate a master table with a combindation of converted and not-converted dates. lubridate::dmy returns an 'NA' for dates that have already been converted.

Next steps:

- Re-run the race_calendar initial function (01) for the tables with old column naming conventions (or just re-name the columns!)

- Combine all the race_calendar tables into a single race_calendar_Master table

- Look at how to combine race results data into master tables



------------------------------
Notes from Tue 25 Apr

I attempted to close out my Coursera exercises, but my access to Jupyter Notebooks has been cut off! Well, at least direct access via Courera has.

I've moved on to sorting out my database, starting with the race calendars and combining them into a master calendar. I think they're almost ready to go, I just need to ensure when they are mashed together that I can separate years cleanly. I discovered the dates are stored as a string and won't therefore responsd to numeric queries. Fortunately, Hadley's 'Lubridate' package was able to convert the 'DD MMMM YYYY' dates to a proper date numeric using the 'dmy' function. I also found that Lubridate's 'month' and 'year' functions are great at extracting elements of the numeric date entry, with formatting for taking out exactly what you want (for example - getting 'January' or 'Jan' instead of '01'.

I also discovered a MySQL function for closing open queries, but leaves the connection to the database and table open:

# Close open queries (doesn't close the connection - very useful)
dbClearResult(dbListResults(conn_local)[[1]])


Next steps:

- Perhaps write the Lubridate conversion straight into the initial race_calendar scrape function (01a_function_initial_CN_calendar.R).

- Check the data in the existing race_calendar tables

- Convert all the start_date and end_date values in the existing calendars (although I could do this when they're all combined)

- Combine all the race_calendar tables into a single race_calendar_Master table

- Look at how to combine race results data into master tables 


------------------------------
Notes from Thu 20 Apr & Sat 22 Apr

On both days I've simply progressed through the Week 4 exercises on my MySQL Database course.


------------------------------
Notes from Tue 18 Apr

I've mostly continued on the datbase week 4 exercises. At the same time I ran the obtain race weblink function to complete the final years (2016 and 2017[partial]). It's now possible to combine all of the race weblink tables into a single master table - noting that I would need to write and update script to go and get new race weblinks as the year (currently 2017) progresses.

I'm now working on the script that takes the raec weblinks and goes and scrapes all the race result tables. The biggest challenge is working out how I'm going to store this in the database. Ideally I would like a single master table for the primary race result, another master table for various points outcomes and that sort of thing. In theory I could ignore GC tables (as I should be able to calculate this from individual stage times), but it's probably prudent to collect this data for validation against my own calculations later on. There's almost certainly going to be some weird anomalies in GC times thanks to corrections and adjustments by race officials.

Next steps:

- Complete database Week 4

- Combine race calendars into a master table

- Look at how to combine race results data into master tables 


------------------------------
Notes from Mon 17 Apr

On Saturday (15th) I tried remote connecting from Maleny (Phil & Brenda's house) but failed. The Study PC (database server host) was suspended when I got home so I think I didn't really have the ability. Need to test again when I'm sure the computer is still on!

This morning I believe I've sorted out the 'text_clean' function. I've inserted the 'magrittr' function to allow me to pipe my various text operators into each other. I'm now running a complete re-scrape of the race calendars (2005 to 2017). It is important to have this little function sorted as I'll be using it repeatedly when I import race data.

I've also done a little bit on Week 4 of my database course. I need to close this out. Week 5 to do as well.

Next steps:

- Complete database Week 4

- Combine race calendars into a master table

- Look at how to combine race results data into master tables




------------------------------
Notes from Thu 13 Apr

I think I've successfully set up the ability to remotely connect in to my MySQL server. The laptop connects. I need to test it offsite to see if there are firewall issues.

Study Computer IP Address
192.168.1.1   
Port  3306

Next steps:

- Test remote connection to database (from Maleny)

- Start implementation of new database relational schema



------------------------------
Notes from Thu 06 Apr

Haven't been able to replicate the error from Tuesday. It was possibly just a HTTP connection error. I should look for code that test for a connection and is able to move on if there isn't on (possibly producing a warning). I think there's some in the UnConf::Ozdata work.

URL check courtesy of the RCurl package
   RCurl::url.exists(race_url)

I've inserted the above RCurl function into GetRaceWebLinks to test it out.

The following does a check. Returns nothing if TRUE. Returns and error if FALSE
   assertthat::assert_that(nrow(calendar_cn) < 0)

Next steps:

- Sort out remote connection to database

- Design database relational schema


------------------------------
Notes from Tue 04 Apr

I seem to have the 02_function mostly sorted. I've just tried running it (getRaceWeblinks) over 2005:2017, but it failed in the middle of 2009 (44% mark), with the following error:

Error: failed to load HTTP resource

The good news is that I sorted out plenty of errors, cleaned out a lot of uncessary code (from 02_function) as well as setting up and single text_clean function to uniformly deal with special characters.


Next steps

- Work out why 02_function failed on the 2009 calendar.



------------------------------
Notes from Mon 03 Apr

I spent this morning updating the calendar scraping function. The latest issue appears to be with another special character, this time the uptick (\91) {there has to be an official name for this problem character} contained within the 2011 calendar for: The Nor\92easter \91Cross 2011 whic is in September.

[UPDATE!]: I believe I've sorted it. The character is called 'Okina' (Hawaiian) and can be removed with the following stange regex expression:  [^[:alnum:]///' ]

gsub("[^[:alnum:]///' ]", "", races_master[i, "race_details"])


I inserted the text progress bar into the calendar and obtain race results functions as well.

Next steps

- Sort out how to deal with this latest problem special character.

- potentially stick all of the 'cleaning' actions into a single function, so this can be used uniformly across all of my cycling code.




------------------------------
Notes from Sun 02 Apr

I've returned to work on my TDF_Predict project after taking a week to focus on the BURGr UnConf 2017. The event was great and I learned an enormous amount. 


This morning I finished updates to initial calendar function and I believe I have all of the race calendars stored in the database.

I'm now moving on to the race results weblink function, which has errors. That's a job for tomorrow.





------------------------------
Notes from Thu 23 Mar

Same again - still working through Coursera database course. It's been really beneficial. Today I finished off my writing some queries in R looking at my Pro Cyling database. Everything seems to be working well! My practice file is:

170323 - Practice ProCycling Database Queries.R



------------------------------
Notes from Tue 21 Mar

Today I continued working through the database course on Coursera. Last week I learnt about Entity Diagrams and Relational Schema (how to interpret and create them) and this week I am getting into MySQL commands.



------------------------------
Notes from Mon 20 Mar

Super painful. I'm trying to sort out the non-UTF8 character problem still. In the 2017 race calendar, the first problem race is on row 14:

"Challenge Mallorca Trofeo Porreres \96 Felanitx \96 Ses Salines \96 Campos"

The problem is the 'em-dash' which isn't apparently UTF-8. I've built a replacement into the 'removePainfulCharacters' function, but I don't like it. Even once I've done that, the database write function throws up the same error - apparently picking out the " " space in the same row 14 string. Annoying!!

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R', specifically trying to sort out further non-UTF8 characters.

- Create an ER Diagram and Relational Schema for the Pro Cycling database.







------------------------------
Notes from Thu 16 Mar

I haven't made a great deal of progress with fixing the problem characters in the 'race_description' column of the calendar tables. I've decided to insert the 'try()' function at the start of the 'dbwriteTables' function and run through the remainding calendars. There's still plenty of problem characters. 

I did spend a bit of productive time familiarising myself with Hadley's lubridate package, which helps with working with dates. I've written a quick function 'setDateFormat' to correct and update the 'start_date' and 'end_date' columns in the calendars. 

I also had a quick look at Hadley's 'broom' package, which is used to clean up the output from predictive modelling. It neatly arranges output models into tables which can be used for further analysis and easier visualisation.

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R'



------------------------------
Notes from Mon 13 Mar

I'm still having problems with certain characters in the race_details column. I fixed the apostophe issue by creating a copy of the removeDiscritics function. In the 2009 calendar I've now run into a problem with inverted commas (") in the 253rd column: 3 Giorni  Internazionale open "Citt\E0\A0 di Pordenone"

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R'




------------------------------
Notes from Mon 13 Mar

I'm continuing to have problems sorting out characters in the calendar function. The first problem event is the following in the 2009 calendar, but there are more. I think the issue is with special/reserved characters, such as quotes and inverted commas. 

Tour of America\92s Dairyland
<a href="/races/tour-of-americas-dairyland-ne">Tour of America\92s Dairyland</a>

Next steps

- Continue working on problems with '01a_function_initial_CN_calendar.R'


------------------------------
Notes from Sun 12 Mar

I had a short play with dplyr this afternoon while Liz was out for a run. The big success was working out how to summarise a value using multiple factor variables. The key was listing a number of variables in the one 'group_by' function dplyr.

e.g.

colnames(df) -> c("State", "Product", "Month", "Sales")

my_summary -> df %>%
   group_by(Month, Product) %>%
   summarise(sum(Sales))

knitr::kable(df)

Here's the list of packages from Hadley's slide presentation:


Import: readr, readxl, xml2, DBI

Tidy: tibble; tidyr

Transform: dplyr, forcats, hns, stringr, lubridate

Visualise: ggplot2

Model: broom

Program: purr, magrittr


------------------------------
Notes from Sat 11 Mar

I've needed to take a step back. I've realised I need the complete list of race calendars written to the database. This helped me realise that my calendar functions (01a & 01b) were writing .csv files instead of the database. I've cleaned these both up and I'm now running the scripts to extract the full race calendars with race weblinks and write them to the database. 

It's interesting to see my script from just over a month ago. It's evident I'm making progress in terms of writing more efficiently and in noting what I'm doing. 

I'm having problems with the '02_function_obtain_event_race_result_weblinks.R' file

I've enrolled in a Coursera course on 'Managing Big Data with MySQL':
https://www.coursera.org/learn/analytics-mysql/home/welcome

Next steps

- Continue working on problems with '02_function_obtain_event_race_result_weblinks.R'

- Continue working through race results functions. 

- Look at running 'GetAllRacesInAYear' function and scraping all years to database. Need to check on the format and data being written to the race weblinks tables.

- Consider how the tables might be amalgamated. Not easy given their heterogeneous nature. 


------------------------------
Notes from Thu 9 Mar

I had a quick go at using some of the dataframe arranging functions in the dplyr package. I went to my first BURGr Meetup last night and had a great time.
(tidyverse, magrittr, dplyr)
This is of course massively easier than some of the base R filtering techniques I've been using!

I was able to fix the issues with the 03_function_race_results_table.R' which is called out as the 'write_race_results_tables' function. I now need to go and scrape all of the race calendars from Cycling News using the 'GetRaceWebLinks' function from the '02_function_....' file.

Next steps

- Continue working through race results functions. 

- Look at running 'GetAllRacesInAYear' function and scraping all years to database. Need to check on the format and data being written to the race weblinks tables.

- Consider how the tables might be amalgamated. Not easy given their heterogeneous nature. 

- Look for Coursera (or MOOC) MySQL database course


------------------------------
Notes from Mon 06 Mar

I've had a reasonable session working through the race results functions and converting them across to writing to the database. Unfortunately I've been reminded the race result tables don't kick in until at after 2007. I was able to successfully update 'GetRaceWebLinks' function from the '02_function_....' file. 

Next steps

- Continue working through race results functions. Specifically, I'm up to the writing of tables in the '03_function_race_results_table.R' which is called out as the 'write_race_results_tables' function in '170211 - Test script for running race result' function.

- Sort out management of open database connections. This is throwing up an error in the '03_function_race_results_table.R' file. 

- Consider how the tables might be amalgamated. Not easy given their heterogeneous nature. 

- Look for Coursera (or MOOC) MySQL database course

------------------------------
Notes from Mon 06 Mar

I finally sorted out the 'rider_master_list' and was able to retrieve all 1572 rows of rider names from the database. The other issue is that I've imported only the riders form the UCI WouldTour teams. I probably also need to import the riders from at least the UCI Pro Continental teams as well.

I couldn't identify any sort of loading error WRT the 'Error : failed to load HTTP resource' that came up yesterday. 

Team categories on the Cycling News website
2005: Four tables: UCI WorldTour (20 teams), Teams (3 teams), UCI Pro Continental Men's (29 teams), UCI Continental Men's (122 teams)
2006: Four tables: UCI WorldTour (18 teams), Teams, (13 teams), UCI Pro Continental Men's (30 teams), UCI Continental Men's (lots)
2007, 2008 & 2009: as above
2010:2017, : Three tables:  UCI WorldTour, UCI Pro Continental Men's & UCI Continental Men's

Next steps

- Work out how to capture team history

- Work through completing the download (and writing to database) of all the race results.


------------------------------
Notes from Sun 05 Mar

I sorted out the issue with team names. The problem was simply related to not having the removeDiscritics function running over the team names. (It was doing riding names only).

I also fixed the riderMasterList error. Again a simple issue. I had the title of the function in the wrong spot. I needed to 'assign' the function to the riderMasterList title.

I've successfully run the entire rider list webscrape, however I got the following error message in what I think is the run of 2016 team tables:

I have done 1 of 18 - gonna sleep 0.24 seconds.
Error : failed to load HTTP resource

In addition: There were 28 warnings (use warnings() to see them)
I have done 2 of 18 - gonna sleep 0.98 seconds.
Error : failed to load HTTP resource

I have done 3 of 18 - gonna sleep 0.85 seconds.

So there's a chance that teams 2 and 3 of 2016 weren't extracted.


Next steps

- Check the error above.

- Work out how to capture team history

- Learn more database skills. e.g. Assignment of PRIMARY KEY. I've got a copy of a SQL database for beginners book that's now in my Calibre library.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.


------------------------------
Notes from Sat 04 Mar 

I believe I've sorted out the function to extract rider details from the CN website. The issue appeared to be the assignment of the data/value to the table to be written in dbWriteTable.

I've also nearly sorted out the Master Rider list, although it's giving me a funny error relating to the LPAREN "}" in my function 'riderMasterList'.

I've just set the getRiderList function to run, extracting all of the rider data from every team from 2005 through to 2017. This should hopefully load all of them into the ProCycling database, with correct table and column names.

Next steps

- Fix 'riderMasterList' function error.

- Work out how to capture team history

- Learn more database skills. e.g. Assignment of PRIMARY KEY. I've got a copy of a SQL database for beginners book that's now in my Calibre library.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.








------------------------------
Notes FROM Thu 02 Mar (written at the end of my morning session, Wed 01 Mar)

#### NOTE: I'm changing my notes date system to specify the date on
#### on which I've written the notes, instead of the date I'm next
#### going to use them. 

I've been dealing with another non UTF-8 character issue, this one in the names of riders.
Sadly, converting the encoding to UTF-8 on a string just inserts all sorts of unusable characters. It appears that the fantastic removeDiscritics function is still the main solution.
Today I inserted what I assume is a Scandanavian character for the letter o. I think I'll just have to keep updating this function as I come across problem characters. With luck, I've got 99% of them. 

Next steps

- Sort out why the 2009 rider list is not writing to the database.

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Learn more database skills. e.g. Assignment of PRIMARY KEY. I've got a copy of a SQL database for beginners book that's now in my Calibre library.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.



------------------------------
Notes for Thu 02 Mar (written at the end of my morning session, Wed 01 Mar)

Good progress on the rider_list script. I was able to:
1. Fix up the latin characters (convert to UTF-8) on the rider names [I can see this is going to be an ongoing issue]
2. Write a combined annual list of years to the database
3. Insert a 'sleep' function to insert delays between webscraping rider data for a team

Next steps

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Attempt to sort out assignment of PRIMARY KEY

- Fix non UTF-8 characters in the race results tables. Race name and race location are still being problematic.

- Start work on database queries. I don't want or need to pull in entire database tables. It will be far more efficient to perform filtering queries.





------------------------------
Notes from my day of coding - Tue 28 Feb

Database Day!

Not as much progress as I had hoped. Finding more problems with unique non UTF-8 characters.

I have managed to sort out the table name and column naming conventions to allow the race calendars to be successfully written to the database. I'm still having problems with assignment of the PRIMARY KEY.




------------------------------
Notes for Thu 02 Mar (written at the end of my morning session, Tue 28 Feb)

I had a more successful time sort out the rider list webscrape today and I think I've got it sorted out. My test runs have been limited to the first few teams in any given year. I had two problems:
1. For some reason I was using the xPath for Nationality when I wanted the UCI ID data; and
2. I needed to find a way to extract just the UCI number. gsub came to the rescue!

I can now generate an overall list of riders for a year. I think it will be useful to put each of these rider lists for each year into the database. I'll still want a master rider list that is a table with only one entry for each rider. 


Next steps

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Once the above master list is created, it should hopefully be straightforward to write this table to the database. The uci.ID for each rider should in theory act as the Primary Key.

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt


------------------------------
Notes for Tue 28 Feb

Woke up late (6am) and have done about 50mins of coding. Didn't really solve my issues with the tables for each team. I'm not extracting the UCI ID cleanly.


------------------------------
Notes for Mon 27 Feb

A decent morning today (Sat 25 Feb). I've managed to finish the script that extracts rider data from the CN website. At the moment I've got a table for each team (in each year) that lists their riding roster, with the following elements: rider.name, rider.link, team.name, dob, nationality & uci.ID

Although I've essentially got all of the data, I now need to organise it into a more useful dataframe that will make later queries much easier. The main trick will be working out how to efficiently deal with all of the rider duplications each year.

It's been gold inserting the Windows Progress Bar. On long (download) scripts it's difficult to know whether it's working it not!

Next steps

- Fix the rider list (for a complete year) component of the script

- Build a master rider list that has only a unique entry for each rider and somehow lists all of the years for which they have ridden.

- Once the above master list is created, it should hopefully be straightforward to write this table to the database. The uci.ID for each rider should in theory act as the Primary Key.


------------------------------
Notes for Sat 25 Feb

Although it didn't feel like I acheived too much from a coding perspective today, I actually made a good amount of ground on the steps required to pull down rider data. It's evident I'm much better at scraping html data now as I can isolate XML nodes and their relevant Values and Attributes pretty quickly. 

After some research, I've decided to use the Cycling News data for rider information. It goes back the year 2000, has pages that follow a logical naming convention and it (importantly) has the UCI ID for each rider (nationality and birthdate combined). 

http://www.cyclingnews.com/teams/2016/
http://www.cyclingnews.com/teams/2016/team-sky/
http://www.cyclingnews.com/riders/peter-kennaugh/

http://www.cyclingnews.com/teams/2000/   # Year
http://www.cyclingnews.com/teams/2000/7up-colorado-cyclist/   # Team
http://www.cyclingnews.com/riders/mike-ley/   # Rider


Next Steps

- Go and get rider information (continue my work on file 170223 - CN Rider Data Initial Script.R)

- Fix database table name error.

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt


------------------------------
Notes for Thu 23 Feb

I've simplified the format of the table IDs so they might be accepted by the MySQL database"

Calendar table:  C.2016
Event ID:        E.2016.001
Race ID:         E.2016.001.R01
Race Table:      E.2016.001.R01.T01

MySQL table column name formats. Doesn't like '#' and 'Rider Name (Country) Team'. I fixed this up, however I'm still getting the following error:

 Error in .local(conn, statement, ...) : 
  could not run statement: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '.002.R01.T01 
( `row_names` text,
	`Pos` text,
	`RiderName.Country.Team` text,
	' at line 1

I suspect it's to do with the use of "." which I want to force in for the naming of Tables. I've been working with these scripts:

170211 - Test script for running race result function.R
03_function_race_results_tables.R

Next Steps

- Fix database table name error.

- Go and get rider information

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt



------------------------------
Notes for Tue 21 Feb

Replaced script writing race tables to .csv files to database files. First run encountered error relating to max length of table name!
Successfully updated Windows progress bar to include label with percentage progress and correct title.


Next Steps

- Fix database table name error. Will likely require a re-think on the table name (which needs to be a unique race identifier).
Error in .local(conn, statement, ...) : 
  could not run statement: Identifier name 'R2015MarsCyclingAustraliaRoadNationalChampionships_01_TbNo_01_Result' is too long

- Go and get rider information

- Splitting up URL requests. See text file:
C:\b_Data_Analysis\Projects\TDF_Predict\Script relating to delaying webscraping requests.txt


------------------------------
Notes for Mon 20 Feb

I've got the full 'GetAllRacesInAYear' function going, which is a huge achievement! Essentially this now allows me to go and extract ALL of the results tables for any of the years captured on the Cycling News website.

I sorted out the 'event.ID' and 'event.name' columns problem by replacing the 'unique' function with !duplicate[n, ]. 

Next steps

- Fix the use of unallowed characters in the write.csv function. A results table with the '*' at the end (as a note) caused the write.csv function to fail.

- Make use of the 'label' component of the winProgressBar function
https://www.r-bloggers.com/progress-bars-in-r-using-winprogressbar/

- Definitely need to progress to writing to the database instead of thousdands of .csv files.



------------------------------
Notes for Sat 18 Feb

A productive morning. I wrote a functions for the race results weblinks in my TDF Predict project and a function to extract all of the Velogames stage results tables for the TDF.

02_function_obtain_event_race_results_weblinks.R
170130 - Velogames Results Table Scrape.R


I'm having a minor amount of problems with my 'GetRaceWebLinks' function. The 'event.ID' and 'event.name' columns are returning the number '3' for every result!

- Continue writing code to go from start to finish. 



------------------------------
Notes from my day of coding - Wed 15 Feb

A big day, and some big wins. The main disappointment is not getting further with my database skills. That will require more effort and time.

I managed to fix up the script to deal with variations in the results tables and how they are named. A bunch of IF statements sorted that out. I'm very close to being able to send R off to webscrape a whole year's worth of race result tables and to put them into .csv files with a unique 'race.ID'. 

On the database front, I've mastered the following:
a. Connecting to the MySQL server
b. Creating a database 'ProCycling'
c. Creating calendar tables 'calendar_cn_2009' etc in the database
d. Executing basic queries and putting the results into a dataframe

Next steps

- Continue writing code to go from start to finish. I feel like I've got the race results tables scraping function essentially sorted. I mostly need to complete the global script to join the calendar table to a loop through all of the races.

- More database skills. I'll soon need to write all of the race results tables somewhere and I'd prefer to write them straight into the database. I need to sort out assigning the Primary Key.



------------------------------
Notes for Tue 14 Feb

Continuing to work on the big kahuna - a full script to go from calendar through to results webscrape and putting it all into tables.

I'm having problems running the getCNresults function on randomly picked event tables with results weblinks. Currently practicing on:
http://www.cyclingnews.com//races/fenioux-france-trophy-isgp3/sprint/results

I realised that not all results tables will be the same (different number of columns and other attributes), so I've made some adjustments to account for that. But I'm having problems.

Next steps

- Continue writing code to go from start to finish. It's hard going (given the scale), but it's useful starting to tease out all the challenges with extracting and sorting the data.



------------------------------
Notes for Tue 14 Feb

Although progress seemed a bit slow at first, I was eventually able to successfully take the Cycling News annual race calendar .csv files and acheive a couple of things:
a. Convert the latin based characters in the race.details column to standard ASCII; and
b. Create new columns (with this clean data) for event.name and event.ID

170213 - CN Calendar - Latin to ASCII and create event name and event ID columns.R
170213 - Function Latin to ASCII.R

The latin conversion was acheived with a function shared on the following website:
http://stackoverflow.com/questions/15253954/replace-multiple-arguments-with-gsub

Next steps

- Write a basic script to follow through race result extraction from start (annual calendar) to end (writing race result tables) 

- Start learning database skills. With MySQL installed and connected, I can probably do all of this from R.
http://cse.unl.edu/~sscott/ShowFiles/SQL/CheatSheet/SQLCheatSheet.html
https://blog.udemy.com/sql-queries/



- It might make sense to have all the results links for a single year in one dataframe and one csv file.



------------------------------
Notes for Mon 13 Feb

I successfully updated my function script that extracts the tables of race results from a cycling news race results webpage. I was reminded that a function can only return one object, so it's not possible to return all the tables as separate dataframes. The solution this time is to write a unique .csv file for each race result table. In the near future, I believe I'll be writing these table to a database. 

170118 - cyclingnews webscrape v1.R

I also had to find a way of uniquely naming  each table. I tidied up the table caption (from XML) extraction and gave each .csv file a name that includes a unique race.ID, the table number (in order on the page) and the caption for each table. This data should in theory all be important when I go to sort out the database files.

Which brings me to some of the unique fields in the database I think will be used as "KEY" fields - important in relational database like SQL. These are fields I think will be important. 

event.ID   # An ID for the cycling event. Must be unique. e.g. TDF2016
race.ID    # This could be a stage, or just a single race  e.g. TDF16S1
rider.ID   # Each rider needs a unique shortform ID. I think the UCI uses Nationality and birthday   e.g.  14031977.AUS
team.ID    #  e.g. Astana 2016 - AST2017


In an extra - I managed to quickly install MySQL and set up a server - and then connect from R, creating a table!

170211 - Sandpit scripts for creating and accessing MySQL database.R


Next steps

- Write a basic script to follow through race result extraction from start (annual calendar) to end (writing race result tables) 

- Start learning database skills. With MySQL installed and connected, I can probably do all of this from R.

- It might make sense to have all the results links for a single year in one dataframe and one csv file.

- Need to work out unique IDs for tables. Race ID, Rider ID, Team ID, etc so that database elements can be called and linked correctly.


------------------------------
Notes for Sat 11 Feb

Success! I was able to isolate the results link infomation, put it in a dataframe, get rid of duplicates. I then extended this to a loop to run through all of the events in the 2016 calendar. I managed to have a csv file written for each race with the results weblinks.

170207 - cyclingnews race webscrape v1.R

Next steps

- It might make sense to have all the results links for a single year in one dataframe and one csv file.

- Need to work out unique IDs for tables. Race ID, Rider ID, Team ID, etc so that database elements can be called and linked correctly.




------------------------------
Notes for Thu 9 Feb

I had my first go at scraping data from an individual race webpage. I used the first race at the top of the 2016 calendar (scraped from CN). I can isolate the XML attribute with links to race results, but isolating the link info by itself and narrowing to just the right race links is proving a bit of a challenge.

170207 - cyclingnews race webscrape v1.R

Next steps

- Continue to refine code to scrape the weblinks for race results pages

- Modify Calendar webscrape to include a dataframe column for the raw 'race' weblink. This looks like it will be important on subsequent weblink searches. i.e. the link without "www.cyclingnews.com" at the front.

- Start thinking about what my main R functions will be and how they will call each other. [Practicing funtions at work might be useful]



------------------------------
Notes for Tue 7 Feb

I spent the entire morning attempting to set up a working GitHub account linked to RStudio. It appears to be working properly. I was able to 'pull' my old ProjectAssignment2 files (Coursera Data Analysis course) successfully down to RStudio. I've unfortunately got plenty to learn about version control and using GitHub with R, but at least I now have a basic backup and version control of my working files. 

Next steps

- Investigate potential free & private Git services (GitHub is $7/mth for private)

- Look at the next step - going to the web links and determining the pathway to race results.

- Download/Install the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.



------------------------------
Notes for Mon 6 Feb

Still going well. Today I successully created 
   # the 'Start.date' and 'End.date' columns in the calendar_cn data.frame
   # Wrote the results of each data frame to a .csv file

I had a good time learning about regular text/string expressions such as 'regexpr', 'nchar' and 'substr'.
I used these to split up the 'Date' field into start and end dates.

So I now have .csv files for all of the years between 2005 and 2016 with full calendar informaiton, including weblinks to each race and start and end date columns.

Next steps:

- Look at the next step - going to the web links and determining the pathway to race results.

- Sort out a Github account and start some version control

- Download/Install the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.


------------------------------
Notes for Sat 4 Feb

Great news! I was able to subset out the weblink for each UCI calendar entry and correctly enter it into my dataframe in the right row. My code is able to successfully ignore this action for rows with no web link.

Next steps:

- Write in loop for the new code to cycle it through all the Cycling News calendar years (easy)

- Adding additional columns to the calendar tables
	* Instead of editing the Date column, create a 'Start' and 'End' column

- Look at the next step - going to the web links and determining the pathway to race results.

- Sort out a Github account and start some version control

- Download/Install the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.



------------------------------
Notes for Thu 2 Feb

Still not having a great deal of luck creating a table with all of the calendar informaiton (weblinks and race data). I did have a bit of success using xpathApply to extract a row element of XML data, however I'm unable to then use this subset to extract the needed information. It's great that I've got a subset of XML data with each row that has all the information I need (in either the xmlValue or Attr data), but it's frustrating that I can't access this yet.

170124 - cyclingnews calendar webscrape v2 - uses xpathApply.R

Next steps:

- Continuation of how to subset xml data to collect race data and web link


------------------------------
Notes for Tue 31 Jan

I didn't have a great deal of luck connecting up the weblinks for races and the complete table for the race calendar. However I did manage to learn some more about extracting XML data using xpathApply. For the first time I was able to extract a neat table from the Velogames results webpages.

I'm still using this to find a neat way of extracting the table information from Cycling News webpages, including the html links.
I think I'm going to have to use a more manual extraction that takes all the data and then combines it into a table in the manner I want.

I also discovered that the next webpages have stage data in a somewhat inconsistent manner than may challenge my approach to automatically generate links to results pages.


Next Steps:  Largely still what is mentioned below for Mon 30 Jan.


------------------------------
Notes for Mon 30 Jan

I was able to extract the web link information for each race from the calendar html data.
The bad news is that not every race in the table has its own page and therefore a link is missing for some race entries.
This means the length of the web link list is different to the number of rows in the table and they don't match.
I'm therefore currently looking at ways to extract the web link data at the same time the table is read into R.

Next steps

- Finding the best way to extract the web links for each race and add it to the calendar data frame.

- Sort out a Github account and start some version control

- Download/Install the VIM text editor

- Start thinking about what my main R functions will be and how they will call each other.


------------------------------
Notes for Sat 28 Jan

Good news! I've written the code to go and get the UCI cycling race table from Cycling News for year 2005 through 2016 (or whatever years I want).
I also inserted code to cleanup the Date column (removing all the uncessary text). 

Next Steps

- Adding additional columns to the calendar tables
	* Instead of editing the Date column, create a 'Start' and 'End' column
	* Add a column for the (anticipated) race results web page
		Two ways to do this: a. educated guess; and b. draw link info from html data

- Look at what other race details would be useful (distance, weather, elevation, altitude, etc)


Note: There is text in the race table that has details of the web address for the results

http://www.cyclingnews.com/races/santos-tour-down-under-2010/stages/
http://www.cyclingnews.com/races/santos-tour-down-under-2010/stage-6/results/


------------------------------
Notes for Wed 25 Jan

I successfully wrote a FOR loop that extracts the tables from the calendar of road races for a particular year on the Cycling News website. These go back to 2005.

Next steps

- Cleaning up the calendar table
	* Multi-day races have messy Date entries
	* Some events have year unique names e.g. - 94th Gent-Weldveren
- Writing a loop that extracts the same table for years 2005 through 2017
- Going to the next level and getting the race weblink for each race


------------------------------
Notes for Tue 24JAN17

I've successfully written a FOR loop that extracts the results tables from a Cycling News race results web page (at least for the Tour Down Under 2016). 


Next steps

- Cleaning up the tables
	* Separating Rider/Country/Team
	* Filling in empty finishing times

- A webscrape that pulls the World Pro Tour race schedule for the year (2016)
	* Race names and attributes (# and names of stages, distances, elevation, date, location, start/finish, weather)
	* Identifies Cycling News naming convention for races
	* http://www.cyclingnews.com/races/calendar/2016/








# Webpages for Cycling news

Santos Tour Down Under
http://www.cyclingnews.com/races/santos-tour-down-under-2016/

Stages
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stages/

Down Under Classic (Adelaide)
http://www.cyclingnews.com/races/santos-tour-down-under-2016/down-under-classic-adelaide/results/
Stage 1
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-1/results/
Stage 2
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-2/results/
Stage 3
http://www.cyclingnews.com/races/santos-tour-down-under-2011/stage-3/results/
Stage 4
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-4/results/
Stage 5
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-5/results/
Stage 6
http://www.cyclingnews.com/races/santos-tour-down-under-2016/stage-6/results/


Gent - Wevelgem
http://www.cyclingnews.com/races/gent-wevelgem-2016/
Results
http://www.cyclingnews.com/races/gent-wevelgem-2016/results/

Paris-Roubaix
http://www.cyclingnews.com/races/paris-roubaix-2016/
Results

Giro d'Italia
http://www.cyclingnews.com/giro-ditalia/
Stages
http://www.cyclingnews.com/giro-ditalia/stages/
Stage 1
http://www.cyclingnews.com/giro-ditalia/stage-1/results/
Stage 2
http://www.cyclingnews.com/giro-ditalia/stage-2/results/
Stage 20
http://www.cyclingnews.com/giro-ditalia/stage-20/results/
Stage 21
http://www.cyclingnews.com/giro-ditalia/stage-21/results/


http://www.cyclingnews.com/races/giro-ditalia-2011/

http://www.cyclingnews.com/races/giro-ditalia-2011/stages/

http://www.cyclingnews.com/races/giro-ditalia-2011/stage-1/results/

http://www.cyclingnews.com/races/giro-ditalia-2011/stage-2/results/

http://www.cyclingnews.com/races/giro-ditalia-2011/stage-21/results/

